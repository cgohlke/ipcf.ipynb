{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pair Correlation Function Analysis of Fluorescence Fluctuations in Big Image Time Series using Python\n",
    "\n",
    "**A tutorial using Python and scientific libraries to implement pair correlation function (pCF) analysis of a big time series of images from fluorescence microscopy on a personal computer.**\n",
    "\n",
    "by [Christoph Gohlke](https://www.lfd.uci.edu/~gohlke/), Laboratory for Fluorescence Dynamics, University of California, Irvine\n",
    "\n",
    "Updated on March 10, 2021\n",
    "\n",
    "Presented at the [Big Data Image Processing & Analysis](http://bigdipa.ccbs.uci.edu/) BigDIPA workshops 2016, 2017, and 2018\n",
    "\n",
    "Supported by the National Institute of Health grant numbers 1R25EB022366-01 and 2P41GM103540-31\n",
    "\n",
    "Released under the creative commons [Attribution 4.0 International](https://creativecommons.org/licenses/by/4.0/) license"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "Spatiotemporal analysis of fluorescence fluctuations of optical microscopy measurements on living samples is a powerful tool that can provide insight into dynamic molecular processes of high biological relevance (Di Rienzo et al., 2016).\n",
    "\n",
    "Pair correlation function (pCF) analysis of fluorescence microscopy image time series acquired using very fast cameras is one of the emerging and promising spatiotemporal techniques. However, it is computationally very expensive, and the analysis of big image data sets used to take hours or be impractical on personal computers.\n",
    "\n",
    "In this tutorial, we use the open-source Python programming language and scientific libraries to compute the pCF analysis of a large time series of fluorescence images acquired with Selective Plane Illumination Microscopy (SPIM).\n",
    "\n",
    "First, we implement a function to calculate the cross-correlation of two time series. We demonstrate the limitations of Python for efficient numerical computations and several ways to overcome them.\n",
    "\n",
    "Next, we implement the pCF analysis of a small simulated image time series and optimize its speed by almost two orders of magnitude.\n",
    "\n",
    "Finally, we use this pCF analysis function to analyze a multi-gigabyte image time series in small, overlapping chunks.\n",
    "\n",
    "This is a tutorial on computing pair correlations in big images. Refer to the references for an introduction to the pair correlation method and how the computed pair correlations can be analyzed and visualized to study molecular flow in cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "To follow this tutorial and run its code, the following prerequisites are needed:\n",
    "\n",
    "#### Familiarity with\n",
    "\n",
    "* pair correlation function analysis of fluorescence fluctuations (e.g. Gratton and Digman lectures)\n",
    "* programming and nD-array computing (e.g. Matlab, numpy)\n",
    "* signal processing, time and frequency domain\n",
    "\n",
    "#### Minimum computer specifications\n",
    "\n",
    "* 64-bit Windows 10, macOS, or Linux based operating system\n",
    "* Core i5 CPU with 4 cores\n",
    "* 8 GB RAM\n",
    "* SSD drive with 50 GB free space\n",
    "* NVIDIA GPU with CUDA drivers\n",
    "* A modern web browser supporting WebSockets\n",
    "* Disabled on-access antivirus scanning for the working and scratch directories\n",
    "\n",
    "#### Python development environment\n",
    "\n",
    "* CPython 3.8 64-bit with development header files and libraries\n",
    "* Python packages: Jupyter, IPython, numpy, scipy, matplotlib, scikit-image, h5py, Cython, dask, numba, and CuPy (optional)\n",
    "* CUDA Toolkit (optional, used for CuPy)\n",
    "* A Python distutils compatible C compiler with OpenMP support: Visual Studio 2019 or gcc\n",
    "\n",
    "#### Tutorial source code and data files\n",
    "\n",
    "* Clone the source code from the [ipcf.ipynb](https://github.com/cgohlke/ipcf.ipynb) repository to a working directory:\n",
    "\n",
    "      git clone https://github.com/cgohlke/ipcf.ipynb\n",
    "\n",
    "* Extract the example data files from the [Simulation_Channel.bin.zip](https://drive.google.com/open?id=1cQZezCCvjdmBsUAju7lbhRVLLpwgA6vx) and [nih3t3-egfp_2.zip](https://drive.google.com/open?id=15KvCU6ntCdBmq6j1QwDZRRFB8uf9wh0V) archives to the ipcf.ipynb directory:\n",
    "\n",
    "      unzip Simulation_Channel.bin.zip -d ipcf.ipynb\n",
    "      unzip nih3t3-egfp_2.zip -d ipcf.ipynb\n",
    "\n",
    "* Open the `ipcf.ipynb` notebook from within the ipcf.ipynb directory, e.g. using locally installed jupyter or a docker image, e.g.:\n",
    "\n",
    "      jupyter-nbclassic ipcf.ipynb\n",
    "      \n",
    "      docker run --rm -p 8888:8888 -v ${PWD}/ipcf.ipynb:/home/jovyan/work/ipcf.ipynb jupyter/scipy-notebook:d990a62010ae\n",
    "\n",
    "#### Configure the runtime environment\n",
    "\n",
    "The notebook depends on a few platform specific variables. Adjust the path to the example data and the path to a scratch directory where large intermediate and output files will be saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import common modules\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "import warnings\n",
    "import threading\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "# limit the number CPUs to use\n",
    "MAXCPUS = min(8, multiprocessing.cpu_count() // 2)\n",
    "\n",
    "# read-only directory where the example data were extracted\n",
    "DATA_PATH = './'\n",
    "\n",
    "# writable directory where large intermediate and output files will be saved\n",
    "# must not be a network drive\n",
    "SCRATCH_PATH = './'\n",
    "\n",
    "# for the BigDIPA workshop cluster\n",
    "if os.path.exists('../../data/02_fcs_computation/'):\n",
    "    DATA_PATH = '../../data/02_fcs_computation/'\n",
    "    SCRATCH_PATH = '../../scratch/'\n",
    "\n",
    "# use sequential MKL to prevent thread oversubscription\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "# os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "import numpy\n",
    "\n",
    "# set compiler and linker arguments for OpenMP\n",
    "if sys.platform == 'win32':\n",
    "    OPENMP_ARGS = '--compile-args=/openmp'\n",
    "else:\n",
    "    OPENMP_ARGS = '--compile-args=-fopenmp --link-args=-fopenmp'\n",
    "\n",
    "# tell numba where to find CUDA NVVM on Windows\n",
    "cuda_path = os.environ.get('CUDA_PATH', 'CUDA not found')\n",
    "if os.path.exists(cuda_path):\n",
    "    os.environ['PATH'] += r';%s\\bin;%s\\nvvm\\bin' % (cuda_path, cuda_path)\n",
    "    os.environ['CUDA_HOME'] = cuda_path\n",
    "\n",
    "# ignore warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# acquire a lock object to force single threaded execution\n",
    "THREADLOCK = threading.RLock()\n",
    "\n",
    "# initialize random number generators\n",
    "random.seed(42)\n",
    "numpy.random.seed(42)\n",
    "\n",
    "# display plots within Jupyter notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# detect if CUDA is available\n",
    "try:\n",
    "    import cupy\n",
    "    SKIP_CUDA = False\n",
    "except ImportError:\n",
    "    SKIP_CUDA = True\n",
    "\n",
    "# record the current time\n",
    "START_TIME = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Challenge\n",
    "\n",
    "The challenge is to **compute the pair correlation function analysis (pCF) of a large time series of images using Python on a personal computer in reasonable time**.\n",
    "\n",
    "Our dataset is a 34.5 GB time series of SPIM images of a biological cell as **35,000 TIFF files of 1024x512 16-bit greyscale samples** each:\n",
    "\n",
    "![image timeseries](image_timeseries.png)\n",
    "\n",
    "As part of molecular flow analysis, we need to **cross-correlate the time series at each pixel of the image with the time series of all its neighbor pixels at a specific radius** after correcting the time series for photobleaching:\n",
    "\n",
    "![image pair correlation function analysis](image_pair_correlation_analysis.png)\n",
    "\n",
    "For a radius of 6 there are 32 neighbor pixels. To analyze an image of 1024x512 pixels, the **number of cross-correlation calculations needed is 16,777,216** (including padding of the border by 6 pixels).\n",
    "\n",
    "We would like to perform the cross-correlation calculations on the dataset in **about 15 minutes**.\n",
    "\n",
    "We have a modern notebook or desktop computer with a minimum of **4 CPU cores, 8 GB RAM, and SSD**, running a 64-bit OS with a scientific Python 3.6 distribution and a C compiler installed.\n",
    "\n",
    "To compute 16,777,216 cross-correlations on 4 CPU cores in 15 minutes, **a single cross-correlation computation must finish in about 215 Âµs** (`4*15*60*1000*1000/16777216`).\n",
    "\n",
    "Here is a **pseudo code for the image pair correlation function (ipCF)** analysis that will later be completed to be the reference implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipcf_reference(image_timeseries, circle_coordinates, bins):\n",
    "    \"\"\"Return pair correlation function analysis of image time series.\n",
    "    \n",
    "    Cross-correlate the time series of each pixel in the image \n",
    "    with all its neighbors at a certain radius and return all \n",
    "    the log-binned and smoothed correlation curves.\n",
    "    \n",
    "    \"\"\"\n",
    "    ntimes, height, width = image_timeseries.shape\n",
    "    npoints = len(circle_coordinates)\n",
    "    radius = circle_coordinates[0, 0]\n",
    "    \n",
    "    result = zeros((height-2*radius, width-2*radius, npoints, len(bins)), \n",
    "                   'float32')\n",
    "    \n",
    "    for y in range(radius, height-radius):\n",
    "        for x in range(radius, width-radius):\n",
    "            a = image_timeseries[:, y, x]\n",
    "            for i in range(npoints):\n",
    "                u, v = circle_coordinates[i]\n",
    "                b = image_timeseries[:, y+v, x+u]\n",
    "                c = correlate(b, a)\n",
    "                result[y-radius, x-radius, i] = smooth(average(c, bins))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# functions that need to be implemented\n",
    "\n",
    "def correlate(a, b):\n",
    "    \"\"\"Return cross-correlation of two arrays.\"\"\"\n",
    "    ...\n",
    "\n",
    "def average(c, bins):\n",
    "    \"\"\"Return averaged chunks of array.\"\"\"\n",
    "    ...\n",
    "\n",
    "def smooth(c):\n",
    "    \"\"\"Return smoothed array.\"\"\"\n",
    "    ...\n",
    "\n",
    "def circle(npoints, radius):\n",
    "    \"\"\"Return circle coordinates.\"\"\"\n",
    "    ...\n",
    "\n",
    "def logbins(size, nbins):\n",
    "    \"\"\"Return up to nbins exponentially increasing integers from 1 to size.\"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "### 1. Implement a fast cross-correlation function\n",
    "\n",
    "In this section, we\n",
    "\n",
    "* review the mathematical definition and some properties of cross-correlation\n",
    "* implement an unnormalized cross-correlation function in pure Python\n",
    "* compare its speed with an implementation in C\n",
    "* try several Python libraries to speed up the cross-correlation calculation: threading, numpy, scipy, numba, numba.cuda, CuPy, and Cython\n",
    "* use the cross-correlation theorem, Cython, and the fft2d C library to implement a very fast circular correlation function\n",
    "\n",
    "Along the way we learn\n",
    "\n",
    "* about CPython and its limitations for numerical computations\n",
    "* to write Python C extensions and interface with C libraries using Cython\n",
    "* to use the Jupyter notebook to interactively manage data, code, visualizations, and explanatory text\n",
    "\n",
    "### 2. Implement pair correlation function analysis of small image time series\n",
    "\n",
    "The techniques learned in the first section are applied to implement the pCF analysis of a small simulated image time series.\n",
    "\n",
    "In this section, we\n",
    "\n",
    "* load and explore a time series of images from a simulation of fluorescence fluctuations\n",
    "* implement functions to normalize, average, and smooth correlation functions for image fluctuation analysis\n",
    "* run the reference implementation of image pCF analysis\n",
    "* visualize the result of the image pCF analysis\n",
    "* optimize the algorithm and implementation of the image pCF analysis for speed\n",
    "\n",
    "### 3. Implement out-of-core pair correlation function analysis of big image time series\n",
    "\n",
    "In this section, we demonstrate some methods to process data that fit on disk but are larger than RAM, aka out-of-core processing. We\n",
    "\n",
    "* interactively browse the 34.5 GB image time series consisting of 35,000 TIFF files\n",
    "* semi-automatically select a subset of the data for further analysis using the scikit-image library\n",
    "* save the selection as a blocked dataset in a HDF5 file\n",
    "* implement a function to correct time series for photobleaching\n",
    "* use the dask library to chop big data in smaller, overlapping blocks/chunks and schedule the analysis of individual blocks\n",
    "* run the photobleaching correction and the image pair correlation function implemented in the second part on overlapping blocks of the big SPIM dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Implement a fast cross-correlation function\n",
    "\n",
    "In this section, we implement a fast 1D cross-correlation function. The goal is to perform a single cross-correlation of two long (2<sup>14</sup>) 1D sequences of real numbers in about 200 Âµs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of cross-correlation\n",
    "\n",
    "The **unnormalized discrete correlation of two sampled functions** $a$ and $b$ as generally defined in signal processing texts (e.g. [numpy.correlate](http://docs.scipy.org/doc/numpy/reference/generated/numpy.correlate.html) and [MATLAB xcorr](https://www.mathworks.com/help/signal/ref/xcorr.html)) is:\n",
    "\n",
    "$$c_{ab}[delay] = \\sum_n a[n+delay] \\cdot conj(b[n])$$\n",
    "\n",
    "This is known as the **sliding dot product**. \n",
    "\n",
    "The above definition is not unique and sometimes correlation may be defined differently (e.g. [Wikipedia](https://en.wikipedia.org/wiki/Cross-correlation)):\n",
    "\n",
    "$$c'_{ab}[delay] = \\sum_n conj(a[n]) \\cdot b[n+delay],$$\n",
    "\n",
    "which is related to the first definition:\n",
    "\n",
    "$$c'_{ab}[delay] = c_{ab}[-delay] = c_{ba}[delay]$$\n",
    "\n",
    "\n",
    "There is also an ambiguity for which range of $delay$s to compute the correlations and how to deal with out of bounds $delay$ values.\n",
    "\n",
    "In this tutorial, $a$, $b$ and $c$ will have the same length. $delay$ will range from negative half length to positive half length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear and circular cross-correlation\n",
    "\n",
    "There are two ways of dealing with **out of bounds $delay$ values**: \n",
    "\n",
    "* for **linear correlation**, the out of bounds indices are set to zero (zero padding):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2]\n",
    "b = [3, 4]\n",
    "\n",
    "c = [    0    * b[0] + a[1 - 1] * b[1],  # delay -1  a[-1]=0\n",
    "     a[0 + 0] * b[0] + a[1 + 0] * b[1]]  # delay 0\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* for **circular correlation**, the out of bounds indices are wrapped around:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2]\n",
    "b = [3, 4]\n",
    "\n",
    "c = [a[0 - 1] * b[0] + a[1 - 1] * b[1],  # delay -1 a[-1]=a[1]\n",
    "     a[0 + 0] * b[0] + a[1 + 0] * b[1]]  # delay 0\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear correlation can be calculated using the circular algorithm by zero padding the input arrays on both sides to twice their lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0, 1, 2, 0]\n",
    "b = [0, 3, 4, 0]\n",
    "\n",
    "c = [a[0 - 1] * b[0] + a[1 - 1] * b[1] + a[2 - 1] * b[2] + a[3 - 1] * b[3],\n",
    "     a[0 + 0] * b[0] + a[1 + 0] * b[1] + a[2 + 0] * b[2] + a[3 + 0] * b[3]]\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of cross-correlation\n",
    "\n",
    "Some properties of the cross-correlation function are relevant for the pair correlation calculations:\n",
    "\n",
    "* According to the **cross correlation theorem**, the cross-correlation ($\\star$) of functions $a(t)$ and $b(t)$ can be calculated using the Fourier transform $\\mathcal{F}\\{\\}$:\n",
    "\n",
    "    $$ a\\star b = \\mathcal{F}^{-1}(\\mathcal{F}\\{a\\} \\cdot (\\mathcal{F}\\{b\\})^*)$$\n",
    "\n",
    "    where $()^*$ denotes the complex conjugate.\n",
    "    For long vectors this method is faster to calculate than the sliding dot product.\n",
    "    \n",
    "\n",
    "* The cross-correlation of functions $a(t)$ and $b(t)$ is equivalent to the **convolution** ($*$) of $a(t)$ and $b^*(-t)$:\n",
    "\n",
    "    $$ a\\star b = a*b^*(-t),$$\n",
    "\n",
    "    where $b^*$ denotes the conjugate of $b$.\n",
    "\n",
    "\n",
    "* For real valued input, the following symmetry applies:\n",
    "\n",
    "$$c_{ab}[delay] = c_{ba}[-delay]$$\n",
    "\n",
    "* The discrete **autocorrelation** of a sampled function is the discrete **correlation of  the function with itself**. It is always symmetric with respect to positive and negative delays:\n",
    "\n",
    "$$c_{aa}[delay] = c_{aa}[-delay]$$\n",
    "\n",
    "* The circular correlation can calculate linear correlation by zero-padding the vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-correlation using pure Python\n",
    "\n",
    "Let's implement the discrete linear correlation function for real input sequences in pure Python using the **sliding dot product** as defined previously:\n",
    "\n",
    "$$c_{ab}[delay] = \\sum_n a[n+delay] * b[n]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_python(a, b, start, stop, delay):\n",
    "    \"\"\"Return dot product of two sequences in range.\"\"\"\n",
    "    sum = 0\n",
    "    for n in range(start, stop):\n",
    "        sum += a[n + delay] * b[n]\n",
    "    return sum\n",
    "\n",
    "\n",
    "def correlate_python(a, b):\n",
    "    \"\"\"Return linear correlation of two sequences.\"\"\"\n",
    "    size = len(a)\n",
    "\n",
    "    c = [0] * size  # allocate output array/list\n",
    "\n",
    "    for index in range(size):\n",
    "        delay = index - size // 2\n",
    "        if delay < 0:\n",
    "            c[index] = dot_python(a, b, -delay, size, delay)\n",
    "        else:\n",
    "            c[index] = dot_python(a, b, 0, size-delay, delay)\n",
    "\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good practice to write tests to verify code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_correlate(correlate_function):\n",
    "    \"\"\"Test linear correlate function using known result.\"\"\"\n",
    "    # even lengths\n",
    "    c = correlate_function([1, 2], [3, 4])\n",
    "    assert list(c) == [4, 11], c\n",
    "    \n",
    "    # uneven lengths\n",
    "    c = correlate_function([1, 2, 3], [4, 8, 16])\n",
    "    assert list(c) == [40, 68, 32], c\n",
    "\n",
    "\n",
    "test_correlate(correlate_python)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tests passed, no exception is raised.\n",
    "\n",
    "Let's time the cross-correlation of two random sequences of length 8192, which are created using [list comprehension](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "A = [random.random()-0.5 for _ in range(2**13)]\n",
    "B = [random.random()-0.5 for _ in range(2**13)]\n",
    "\n",
    "%time c = correlate_python(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation is about **25,000 times slower than desired** (~200 Âµs) for our pair correlation function image analysis task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot auto-correlation\n",
    "\n",
    "Using the [matplotlib](http://matplotlib.org/) 2D plotting library, we plot the auto-correlation of a short random sequence and embed it into the Juyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "def plot_autocorrelation(size=200):\n",
    "    \"\"\"Plot autocorrelation of a random sequence.\"\"\"\n",
    "    a = [random.random()-0.5 for _ in range(size)]\n",
    "    c = correlate_python(a, a)\n",
    "    delays = list(range(-len(a) // 2,  len(a) // 2))\n",
    "\n",
    "    pyplot.figure(figsize=(6, 6))\n",
    "    pyplot.subplot(2, 1, 1)\n",
    "    pyplot.title('random sequence')\n",
    "    pyplot.ylabel('intensity')\n",
    "    pyplot.plot(a, 'g')\n",
    "\n",
    "    pyplot.subplot(2, 1, 2)\n",
    "    pyplot.title('auto-correlation')\n",
    "    pyplot.xlabel('delay')\n",
    "    pyplot.ylabel('correlation')        \n",
    "    pyplot.plot(delays, c, 'r')\n",
    "    \n",
    "    pyplot.tight_layout()\n",
    "    pyplot.show()\n",
    "\n",
    "\n",
    "plot_autocorrelation()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autocorrelation is always **symmetric** with respect to positive and negative delays.\n",
    "\n",
    "For long random sequences, the autocorrelation approaches an **impulse function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactively plot cross-correlation\n",
    "\n",
    "Using [IPython widgets](https://ipywidgets.readthedocs.io/en/latest/), we plot the cross-correlation of two short random sequences with peak, where the peak in sequence $b$ is delayed with respect to sequence $a$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "from ipywidgets import interact, IntSlider, Dropdown\n",
    "\n",
    "\n",
    "def plot_crosscorrelation(size=100):\n",
    "    \"\"\"Interactively plot cross-correlation of signals with delayed peak.\"\"\"\n",
    "    delays = list(range(-size//2,  size//2))\n",
    "    a = [random.random()-0.5 for _ in range(size)]\n",
    "    b = [random.random()-0.5 for _ in range(size)]\n",
    "    \n",
    "    a[size//2] = 10  # add peak in middle of sequence\n",
    "    \n",
    "    def _plot(option, delay):\n",
    "        b_ = b.copy()\n",
    "        b_[size//2 + delay] = 10  # add peak at shifted position\n",
    "        \n",
    "        if option.endswith('b'):\n",
    "            c = correlate_python(a, b_)\n",
    "        else:\n",
    "            c = correlate_python(b_, a)\n",
    "            \n",
    "        pyplot.figure(figsize=(6, 6))        \n",
    "        pyplot.subplot(2, 1, 1)\n",
    "        pyplot.title('random sequences with peak')\n",
    "        pyplot.ylabel('intensity')\n",
    "        pyplot.plot(a, 'g', label='a')\n",
    "        pyplot.plot(b_, 'b', label='b')  \n",
    "        pyplot.ylim([-2, 12])\n",
    "        pyplot.yticks([0, 5, 10])\n",
    "        pyplot.legend(fancybox=True, framealpha=0.5)\n",
    "\n",
    "        pyplot.subplot(2, 1, 2)\n",
    "        pyplot.title('cross-correlation')\n",
    "        pyplot.xlabel('delay')\n",
    "        pyplot.ylabel('correlation')\n",
    "        pyplot.xlim([-size//2, size//2])\n",
    "        pyplot.ylim([-20, 120])\n",
    "        pyplot.yticks([0, 50, 100])\n",
    "        pyplot.plot(delays, c, 'r', label=option)\n",
    "        pyplot.legend(fancybox=True, framealpha=0.5)\n",
    "        \n",
    "        pyplot.tight_layout()\n",
    "        pyplot.show()\n",
    "\n",
    "    interact(_plot, \n",
    "             option=Dropdown(options=['a\\u2605b', 'b\\u2605a']), \n",
    "             delay=IntSlider(value=size//5, min=2-size//2, max=size//2-1,\n",
    "                             continuous_update=False))\n",
    "\n",
    "\n",
    "plot_crosscorrelation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A positive delay of the peak in sequence $b$ with respect to the peak in sequence $a$ shows as a peak at a negative delay in the cross-correlation of a and b ($a \\star b$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-threading\n",
    "\n",
    "Let's try to use Python's [concurrent.futures](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor) module to **run several correlation functions in parallel on multiple CPU cores** within the same process using threads (the smallest sequences of programmed instructions that can be managed independently by the operating system):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "def map_threaded(function, *iterables, max_workers=MAXCPUS, **kwargs):\n",
    "    \"\"\"Apply function to every item of iterable and return list of results.\n",
    "    \n",
    "    Use a pool of threads to execute calls asynchronously.\n",
    "    \n",
    "    \"\"\"\n",
    "    if kwargs:\n",
    "        function = partial(function, **kwargs)\n",
    "    with ThreadPoolExecutor(max_workers) as executor:\n",
    "        return list(executor.map(function, *iterables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time c = map_threaded(correlate_python, [A, A], [B, B])\n",
    "\n",
    "assert c[0] == c[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is **no improvement** over executing the functions sequentially.\n",
    "\n",
    "In CPython, the reference Python implementation we are using, **only one thread can execute Python code at once** due to the **Global Interpreter Lock (GIL)**.\n",
    "\n",
    "Threading is still an appropriate model in Python to run multiple I/O-bound tasks simultaneously. \n",
    "\n",
    "We demonstrate later that Python functions implemented in C can release the GIL and be executed on multiple CPU cores using threads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-correlation using C\n",
    "\n",
    "Let's compare the performance of the pure Python function to an implementation in C.\n",
    "\n",
    "The speed of compiled C code is often used as a reference when comparing single-threaded performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile correlate_c.c\n",
    "\n",
    "/* A linear correlate function implemented in C. */\n",
    "\n",
    "#include <stddef.h>\n",
    "#include <stdlib.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "typedef ptrdiff_t ssize_t;\n",
    "\n",
    "/* Compute dot product of two sequences in range. */\n",
    "double dot_c(double *a, double *b, ssize_t start, ssize_t end, ssize_t delay)\n",
    "{\n",
    "    ssize_t n;\n",
    "    double sum = 0.0;\n",
    "    for (n = start; n < end; n++)\n",
    "        sum += a[n + delay] * b[n];\n",
    "    return sum;\n",
    "}\n",
    "\n",
    "\n",
    "/* Compute linear correlation of two one-dimensional sequences. */\n",
    "void correlate_c(double *a, double *b, double *c, ssize_t size)\n",
    "{\n",
    "    ssize_t index, delay;\n",
    "\n",
    "    for(index = 0; index < size; index++) {\n",
    "        delay = index - size / 2;\n",
    "        if (delay < 0) {\n",
    "            c[index] = dot_c(a, b, -delay, size, delay);\n",
    "        }\n",
    "        else {\n",
    "            c[index] = dot_c(a, b, 0, size-delay, delay);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "/* Time the correlate_c function. */\n",
    "int main()\n",
    "{\n",
    "    ssize_t i;\n",
    "    ssize_t size = 8192;\n",
    "    ssize_t loops = 25;\n",
    "\n",
    "    double *a = (double*)malloc(size * sizeof(double));\n",
    "    double *b = (double*)malloc(size * sizeof(double));\n",
    "    for (i = 0; i < size; i++) {\n",
    "        a[i] = (double)rand()/(double)(RAND_MAX) - 0.5;\n",
    "        b[i] = (double)rand()/(double)(RAND_MAX) - 0.5;\n",
    "    }\n",
    "\n",
    "    for (i = 0; i < loops; i++) {\n",
    "        double *c = (double*)malloc(size * sizeof(double));\n",
    "        correlate_c(a, b, c, size);\n",
    "        free(c);\n",
    "    }\n",
    "\n",
    "    free(a);\n",
    "    free(b);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's [distutils.ccompiler](https://docs.python.org/3/distutils/apiref.html#module-distutils.ccompiler) module can be used to compile and link the C code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils import ccompiler\n",
    "\n",
    "compiler = ccompiler.new_compiler()\n",
    "objects = compiler.compile(['correlate_c.c'], extra_postargs=['-O2'])\n",
    "compiler.link_executable(objects, 'correlate_c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The generated executable can be timed using Jupyter's magick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlate_executable = './correlate_c'\n",
    "t = %timeit -r 1 -q -o ! $correlate_executable\n",
    "\n",
    "print('{:.2f} ms per loop'.format(t.best * 1000 / 25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The C program calculates the correlation about **two orders of magnitudes faster than Python**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python lists\n",
    "\n",
    "So far, we have used Python's built-in list type to store sequences of floating-point numbers.\n",
    "\n",
    "Depending on the Python implementation and platform:\n",
    "\n",
    "* every item of a list is an 8-byte pointer to an object storing the value.\n",
    "\n",
    "* every floating-point number is stored as a 24-byte object.\n",
    "\n",
    "Hence, Python lists are very inefficient for storing large number of homogeneous numerical data:\n",
    "\n",
    "* the numbers are **not stored contiguously** in a Python list.\n",
    "\n",
    "* a Python list of floating-point numbers is about **4x larger than a C array**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "\n",
    "size = 8192\n",
    "alist = [random.random() for _ in range(size)]\n",
    "\n",
    "print('Storage size of Python list: {:>6} bytes'.format(\n",
    "        sys.getsizeof(alist) + sys.getsizeof(alist[0]) * size))\n",
    "\n",
    "print('Storage size of C array:     {:>6} bytes'.format(8 + size * 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Python?\n",
    "\n",
    "So far, we have shown that:\n",
    "\n",
    "* Python built-in lists cannot efficiently store homogeneous numerical data.\n",
    "* Python runs numerical code orders of magnitudes slower than compiled C.\n",
    "* Python code cannot be run in parallel on multiple CPU cores in the same process.\n",
    "\n",
    "Note that this applies to CPython, the Python reference implementation, only. Other Python implementations (pypy, Jython, IronPython) might not have these limitations.\n",
    "\n",
    "#### Why do we consider Python for big data image processing and analysis?\n",
    "\n",
    "There are technical solutions to overcome those limitations:\n",
    "\n",
    "* The **numpy library** provides a **standardized, efficient N-dimensional array object** to store homogeneous numerical data.\n",
    "\n",
    "* **Many third-party libraries** (numpy, scipy, scikit-image, etc.) provide fast implementations of numerical functions operating on numpy arrays.\n",
    "\n",
    "* Python can be **extended using modules written in C**, which can release the GIL.\n",
    "\n",
    "* Python code can be **type annotated and compiled to C code** using Cython.\n",
    "\n",
    "* Python code using a subset of the language synthax can be **just in time compiled to LLVM, CUDA, or OpenCL** and executed on CPU or GPU, e.g. via numba.\n",
    "\n",
    "\n",
    "Putting the limitations into perspective: besides CPU bound numerical calculations, there are many other tasks that are part of an efficient image processing pipeline:\n",
    "\n",
    "* Many tasks are **I/O bound** (load or save data from/to the Internet, hard drive, or databases) and can be efficiently multi-threaded in Python.\n",
    "\n",
    "* Besides threading, there are other **methods to analyze data in parallel**: SIMD, multiprocessing, distributed.\n",
    "\n",
    "* Python can be used to **drive/control/schedule compile and compute tasks**, e.g. generate, compile, and execute C/OpenCL/CUDA code at runtime.\n",
    "\n",
    "\n",
    "As an image analyst or end user of imaging software, Python can be used\n",
    "\n",
    "* as a **glue language** for external libraries or executables written in C, Fortran, R, Java, .NET, Matlab, etc.\n",
    "\n",
    "* for **data munging**, i.e. mapping image and meta-data from a diversity of formats (raw binary, html, CSV, TIFF, HDF5, etc.) and sources (file system, databases, http, ftp, cloud storage) into more convenient formats.\n",
    "\n",
    "* as a **scripting language for imaging software** such as ZEN, Î¼Manager, CellProfiler, MeVisLab, ArcGIS, Amira, ParaView, VisIt, GIMP, Blender, OMERO, BisQue, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-correlation using Numpy\n",
    "\n",
    "Besides an efficient N-dimensional array object, the [numpy](http://www.numpy.org/) library provides useful, optimized functions operating on the arrays, including random number capabilities and a [correlate](http://docs.scipy.org/doc/numpy/reference/generated/numpy.correlate.html) function.\n",
    "\n",
    "Let's redefine the correlate and test functions using numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "\n",
    "def correlate_numpy(a, b):\n",
    "    \"\"\"Return linear correlation of two one-dimensional arrays.\"\"\"\n",
    "    return numpy.correlate(a, b, mode='same')\n",
    "\n",
    "\n",
    "def test_correlate(correlate_function, **kwargs):\n",
    "    \"\"\"Test correlate function using known results.\"\"\"\n",
    "    c = correlate_function(numpy.array([1., 2., 3.]), \n",
    "                           numpy.array([4., 8., 16.]), **kwargs)\n",
    "    assert numpy.allclose(c, [40., 68., 32.]), c\n",
    "\n",
    "    c = correlate_function(numpy.array([1., 2., 3., 4.]), \n",
    "                           numpy.array([5., 6., 7., 8.]), **kwargs)\n",
    "    assert numpy.allclose(c, [23.0, 44.0, 70.0, 56.0]), c\n",
    "\n",
    "\n",
    "test_correlate(correlate_numpy)\n",
    "\n",
    "A = numpy.random.random(2**13) - 0.5\n",
    "B = numpy.random.random(2**13) - 0.5\n",
    "\n",
    "%timeit correlate_numpy(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on how the numpy library was compiled, the **numpy.correlate function is several times faster than our implementation in C**.\n",
    "\n",
    "Numpy uses an optimized version of the dot product (from the BLAS library) for calculating the sliding dot product.\n",
    "\n",
    "The storage size of the numpy array is close to a C array. The overhead of less than 100 bytes matters only for scalar values and small arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Storage size of numpy array: {} bytes'.format(sys.getsizeof(A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy multi-threaded\n",
    "\n",
    "Many numpy functions release the GIL and can be run in parallel on multiple CPU cores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit map_threaded(correlate_numpy, [A, A], [B, B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, depending on the numpy build options, some functions might use threads internally, which can cause oversubscription and parallel execution to be slower than sequential. In such cases, when using numpy built with Intel's MKL library, set the `MKL_NUM_THREADS` environment variable to 1 to disable the libraries internal use of threads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-correlation using Cython\n",
    "\n",
    "[Cython](http://cython.org/) is an optimizing static compiler for both the Python programming language and the extended Cython programming language. \n",
    "\n",
    "Cython makes writing C extensions for Python and interfacing with numpy arrays and C libraries easy. Unlike numba, Cython requires an external, Python compatible C compiler.\n",
    "\n",
    "Cython integrates well with the Jupyter Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use Cython to implement the sliding dot product cross-correlation by\n",
    "\n",
    "* **type annotating** the Python code\n",
    "* **releasing the GIL**\n",
    "* using **[typed memoryviews](http://cython.readthedocs.io/en/latest/src/userguide/memoryviews.html)** to access data in numpy arrays\n",
    "* **compiling the code to machine code** via Python C extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython --compile-args=-O2\n",
    "#\n",
    "#cython: boundscheck=False\n",
    "#cython: wraparound=False\n",
    "\n",
    "import numpy\n",
    "\n",
    "\n",
    "cdef double dot_cython(double[::1] a, double[::1] b,\n",
    "                       ssize_t start, ssize_t end, ssize_t delay) nogil:\n",
    "    \"\"\"Return dot product of two sequences in range.\"\"\"\n",
    "    cdef ssize_t n\n",
    "    cdef double sum\n",
    "\n",
    "    sum = 0.0\n",
    "    for n in range(start, end):\n",
    "        sum += a[n + delay] * b[n]\n",
    "    return sum\n",
    "\n",
    "\n",
    "def correlate_cython(double[::1] a not None, double[::1] b not None):\n",
    "    \"\"\"Return linear correlation of two one-dimensional arrays.\"\"\"\n",
    "    cdef ssize_t size, delay, index\n",
    "\n",
    "    size = len(a)\n",
    "    result = numpy.empty(size, dtype='float64')\n",
    "\n",
    "    # numpy array objects cannot be accessed in a nogil section\n",
    "    # use a Cython typed memoryview instead\n",
    "    cdef double[::1] c = result\n",
    "\n",
    "    with nogil:\n",
    "        for index in range(size):\n",
    "            delay = index - size // 2\n",
    "            if delay < 0:\n",
    "                c[index] = dot_cython(a, b, -delay, size, delay)\n",
    "            else:\n",
    "                c[index] = dot_cython(a, b, 0, size-delay, delay)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_correlate(correlate_cython)\n",
    "\n",
    "%timeit correlate_cython(A, B)\n",
    "%timeit map_threaded(correlate_cython, [A, A], [B, B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cython implementation is about **as fast as the C implementation**. Since the function releases the GIL, it can efficiently run in parallel on multiple CPU cores using multi-threading.\n",
    "\n",
    "Cython's code analysis can be helpful in type annotating and optimizing code. Try it using `%%cython --annotate` at the top of the previous cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Cython with OpenMP\n",
    "\n",
    "[OpenMP](http://openmp.org/wp/) (Open Multi-Processing) is an application programming interface (API) that supports multi-platform **shared memory multiprocessing** programming in C, C++, and Fortran.\n",
    "\n",
    "Cython's [`prange`](http://cython.readthedocs.io/en/latest/src/userguide/parallelism.html) function is implemented using OpenMP's \"`#pragma omp parallel for`\" directive.\n",
    "\n",
    "We instruct the C compiler to use OpenMP by specifying extra platform specific compiler and linker arguments, which were defined at the beginning of the notebook in the `OPENMP_ARGS` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython --compile-args=-O2  $OPENMP_ARGS\n",
    "#\n",
    "#cython: boundscheck=False\n",
    "#cython: wraparound=False\n",
    "\n",
    "import numpy\n",
    "from cython.parallel import prange, parallel\n",
    "\n",
    "\n",
    "cdef double dot_cython(double[::1] a, double[::1] b,\n",
    "                       ssize_t start, ssize_t end, ssize_t delay) nogil:\n",
    "    \"\"\"Return dot product of two sequences in range.\"\"\"\n",
    "    cdef ssize_t n\n",
    "    cdef double sum\n",
    "\n",
    "    sum = 0.0\n",
    "    for n in range(start, end):\n",
    "        sum += a[n + delay] * b[n]\n",
    "    return sum\n",
    "\n",
    "\n",
    "def correlate_cython_omp(double[::1] a not None, double[::1] b not None,\n",
    "                         int num_threads=0):\n",
    "    \"\"\"Return linear correlation of two one-dimensional arrays.\"\"\"\n",
    "    cdef ssize_t size, delay, index\n",
    "\n",
    "    size = a.size\n",
    "    result = numpy.empty(size, dtype='float64')\n",
    "    cdef double[::1] c = result\n",
    "\n",
    "    with nogil, parallel(num_threads=num_threads):\n",
    "        for index in prange(size):\n",
    "            delay = index - size // 2\n",
    "            if delay < 0:\n",
    "                c[index] = dot_cython(a, b, -delay, size, delay)\n",
    "            else:\n",
    "                c[index] = dot_cython(a, b, 0, size-delay, delay)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_correlate(correlate_cython_omp)\n",
    "\n",
    "%timeit correlate_cython_omp(A, B)\n",
    "%timeit map_threaded(correlate_cython_omp, [A, A], [B, B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the number of CPU cores, this function is several times faster than the previous implementation, but it can no longer be efficiently multi-threaded because the function already uses all CPU cores via OpenMP. \n",
    "\n",
    "This implementation is slightly faster than using the `vsldCorrExec1D` function in direct mode from Intel's Math Kernel Library (MKL) (not shown)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just-in-time compile Python code using Numba\n",
    "\n",
    "The [numba](http://numba.pydata.org/) library can **generate optimized machine code from Python code** using the LLVM compiler infrastructure. No external C compiler is required.\n",
    "\n",
    "We can simply decorate the pure Python functions with [`numba.jit`](http://numba.pydata.org/numba-doc/latest/user/jit.html) and use numpy array instead of lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import numba\n",
    "\n",
    "\n",
    "@numba.jit(nogil=True)\n",
    "def dot_numba(a, b, start, stop, delay):\n",
    "    \"\"\"Return dot product of two sequences in range.\"\"\"\n",
    "    sum = 0.0\n",
    "    for n in range(start, stop):\n",
    "        sum += a[n + delay] * b[n]\n",
    "    return sum\n",
    "\n",
    "\n",
    "@numba.jit\n",
    "def correlate_numba(a, b):\n",
    "    \"\"\"Return linear correlation of two one-dimensional arrays.\"\"\"\n",
    "    size = len(a)\n",
    "    \n",
    "    c = numpy.empty(size, 'float64')  # allocate output numpy array\n",
    "    \n",
    "    for index in range(size):\n",
    "        delay = index - size // 2\n",
    "        if delay < 0:\n",
    "            c[index] = dot_numba(a, b, -delay, size, delay)\n",
    "        else:\n",
    "            c[index] = dot_numba(a, b, 0, size-delay, delay)\n",
    "\n",
    "    return c\n",
    "\n",
    "\n",
    "test_correlate(correlate_numba)\n",
    "\n",
    "%timeit correlate_numba(A, B)\n",
    "%timeit map_threaded(correlate_numba, [A, A], [B, B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling Python code with numba achieves **speed comparable to C**.\n",
    "\n",
    "The function cannot be run in parallel on CPU cores even though the inner `dot` function releases the GIL. The loops need to be factored out to a function that does not hold the GIL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelized numba code\n",
    "\n",
    "To further improve the Numba implementation of the correlation function, we\n",
    "\n",
    "1. move the code of the outer loop `for index in range(size):` into its own function named `correlate_numba_jit` and release the GIL\n",
    "\n",
    "2. use the [`numba.prange`](http://numba.pydata.org/numba-doc/latest/user/parallel.html) function to parallelize the outer loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import numba\n",
    "\n",
    "\n",
    "@numba.jit(nogil=True)\n",
    "def dot_numba(a, b, start, stop, delay):\n",
    "    \"\"\"Return dot product of two sequences in range.\"\"\"\n",
    "    sum = 0.0\n",
    "    for n in range(start, stop):\n",
    "        sum += a[n + delay] * b[n]\n",
    "    return sum\n",
    "\n",
    "\n",
    "@numba.jit(nogil=True, parallel=True)\n",
    "def correlate_numba_jit(c, a, b, size):\n",
    "    \"\"\"Compute linear correlation of two arrays using sliding-dot product.\"\"\"\n",
    "    \n",
    "    for index in numba.prange(size):\n",
    "        delay = index - size // 2\n",
    "        if delay < 0:\n",
    "            c[index] = dot_numba(a, b, -delay, size, delay)\n",
    "        else:\n",
    "            c[index] = dot_numba(a, b, 0, size-delay, delay)\n",
    "\n",
    "\n",
    "def correlate_numba_parallel(a, b):\n",
    "    \"\"\"Return linear correlation of two one-dimensional arrays.\"\"\"\n",
    "    size = len(a)\n",
    "    c = numpy.empty(size, 'float64')\n",
    "    with THREADLOCK:\n",
    "        correlate_numba_jit(c, a, b, size)\n",
    "    return c\n",
    "\n",
    "\n",
    "test_correlate(correlate_numba_parallel)\n",
    "\n",
    "%timeit correlate_numba_parallel(A, B)\n",
    "%timeit map_threaded(correlate_numba_parallel, [A, A], [B, B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just-in-time compile Python code to CUDA using Numba\n",
    "\n",
    "The [numba.cuda](https://numba.pydata.org/numba-doc/dev/cuda/index.html) decorator can translate Python functions into PTX code, which execute on the [CUDA](https://developer.nvidia.com/cuda-zone) hardware, e.g. a NVidia graphics card with thousands of cores.\n",
    "\n",
    "In the **CUDA execution model**, a **kernel** function is executed once by each thread on a **grid of blocks of threads**. The grid can be 1, 2, or 3 dimensional. The threads within each block can synchronize and share memory. Thread blocks can execute independently in any order. The kernel function can determine the absolute position of the current thread in the entire grid of blocks.\n",
    "\n",
    "![cuda_grid](cuda_grid.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import numba.cuda\n",
    "\n",
    "\n",
    "@numba.cuda.jit(device=True, inline=True)\n",
    "def dot_cuda(a, b, start, stop, delay):\n",
    "    \"\"\"Return dot product of two sequences in range.\"\"\"\n",
    "    sum = 0.0\n",
    "    for i in range(start, stop):\n",
    "        sum += a[i + delay] * b[i]\n",
    "    return sum\n",
    "\n",
    "\n",
    "@numba.cuda.jit()\n",
    "def correlate_cuda_kernel(c, a, b, size):\n",
    "    \"\"\"CUDA kernel to compute linear correlation of two arrays.\"\"\"\n",
    "\n",
    "    # global position of the thread in the 1D grid\n",
    "    index = numba.cuda.grid(1)\n",
    "\n",
    "    if index < size:\n",
    "        delay = index - size // 2\n",
    "        if delay < 0:\n",
    "            c[index] = dot_cuda(a, b, -delay, size, delay)\n",
    "        else:\n",
    "            c[index] = dot_cuda(a, b, 0, size-delay, delay)\n",
    "\n",
    "\n",
    "def correlate_numba_cuda(a, b):\n",
    "    \"\"\"Return linear correlation of two one-dimensional arrays.\"\"\"\n",
    "    size = a.size\n",
    "    c = numpy.zeros(size, 'float64')\n",
    "\n",
    "    # launch the CUDA kernel\n",
    "    threadsperblock = 32\n",
    "    blockspergrid = (size + (threadsperblock - 1)) // threadsperblock\n",
    "\n",
    "    correlate_cuda_kernel[blockspergrid, threadsperblock](c, a, b, size)\n",
    "\n",
    "    return c\n",
    "\n",
    "\n",
    "if not SKIP_CUDA:\n",
    "    test_correlate(correlate_numba_cuda)\n",
    "\n",
    "    %timeit correlate_numba_cuda(A, B)\n",
    "    %timeit map_threaded(correlate_numba_cuda, [A, A], [B, B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this on a 1920 core GPU is about 20 times faster than on a single core CPU device. For longer input arrays, the GPU will be significant faster (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switching to frequency domain\n",
    "\n",
    "So far, we have calculated the cross-correlation in the **time domain** using the **sliding dot product**. \n",
    "\n",
    "For longer sequences, it is more efficient to use the **cross-correlation theorem** to calculate the cross-correlation in the **frequency domain** using Fourier transforms $\\mathcal{F}\\{\\}$:\n",
    "\n",
    "$$ a\\star b = \\mathcal{F}^{-1}(\\mathcal{F}\\{a\\} \\cdot (\\mathcal{F}\\{b\\})^*)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-correlation using Scipy's convolution function\n",
    "\n",
    "The [scipy](https://www.scipy.org/) library provides many efficient numerical routines such as numerical integration and optimization.\n",
    "\n",
    "The [`scipy.signal.fftconvolve`](http://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.fftconvolve.html) function uses zero-padding and the **Fast Fourier Transform (FFT)** according to the convolution theorem to calculate the convolution of two arrays. \n",
    "\n",
    "Recall that the cross-correlation of functions $a(t)$ and $b(t)$ is equivalent to the **convolution** ($*$) of $a(t)$ and $b^*(-t)$:\n",
    "\n",
    "$$ a\\star b = a*b^*(-t),$$\n",
    "\n",
    "It means that **correlation can be calculated using convolution by reversing the second input array**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "\n",
    "\n",
    "def correlate_scipy(a, b):\n",
    "    \"\"\"Return circular correlation of two one-dimensional arrays.\"\"\"\n",
    "    return scipy.signal.fftconvolve(a, b[::-1], 'same')\n",
    "\n",
    "\n",
    "test_correlate(correlate_scipy)\n",
    "\n",
    "%timeit correlate_scipy(A, B)\n",
    "%timeit map_threaded(correlate_scipy, [A, A], [B, B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about **an order of magnitude faster** and **scales much better with larger array sizes** (not shown) than the multi-threaded Cython implementation of the sliding dot product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circular cross-correlation using FFT\n",
    "\n",
    "Let's implement a circular correlation function using [numpy's FFT functions](https://docs.scipy.org/doc/numpy/reference/generated/numpy.fft.fft.html) according to the **cross-correlation theorem**:\n",
    "\n",
    "$$ a\\star b = \\mathcal{F}^{-1}(\\mathcal{F}\\{a\\} \\cdot (\\mathcal{F}\\{b\\})^*)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.fft\n",
    "\n",
    "\n",
    "def correlate_fft(a, b):\n",
    "    \"\"\"Return circular correlation of two one-dimensional arrays.\"\"\"\n",
    "    # forward DFT\n",
    "    a = numpy.fft.rfft(a)\n",
    "    b = numpy.fft.rfft(b)\n",
    "    # multiply by complex conjugate\n",
    "    a *= b.conj()\n",
    "    # reverse DFT\n",
    "    c = numpy.fft.irfft(a)\n",
    "    # shift\n",
    "    c = numpy.fft.fftshift(c)\n",
    "    return c\n",
    "\n",
    "\n",
    "%timeit correlate_fft(A, B)\n",
    "%timeit map_threaded(correlate_fft, [A, A], [B, B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the **circular correlation function can calculate the linear correlation by zero-padding the arrays** (to twice the size for even length arrays):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "c = correlate_fft(numpy.pad(A, A.size//2, mode='constant'), \n",
    "                  numpy.pad(B, B.size//2, mode='constant'))\n",
    "\n",
    "# remove zero-padding from result\n",
    "c = c[A.size//2: -A.size//2]\n",
    "\n",
    "# compare to linear correlation\n",
    "assert numpy.allclose(c, correlate_numpy(A, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, circular correlation should only be used to analyze cyclic functions. However, later we demonstrate that for our specific application the results obtained from circular correlation do not significantly differ from linear correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circular cross-correlation using CUDA FFT\n",
    "\n",
    "[CuPy](https://docs-cupy.chainer.org) is an implementation of a NumPy-compatible multi-dimensional array on CUDA. \n",
    "\n",
    "To run the FFT based circular correlation function on a GPU, we\n",
    "\n",
    "* move the input numpy arrays to the current GPU device using `cupy.asarray()`\n",
    "* use FFT functions from `cupy.fft` instead of `numpy.fft`\n",
    "* move the result array from the GPU device to the host using `cupy.asnumpy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "if not SKIP_CUDA:\n",
    "    import cupy.fft\n",
    "\n",
    "\n",
    "def correlate_cufft(a, b):\n",
    "    \"\"\"Return circular correlation of two one-dimensional arrays.\"\"\"\n",
    "    \n",
    "    # move arrays to the current GPU device\n",
    "    a = cupy.asarray(a)\n",
    "    b = cupy.asarray(b)\n",
    "\n",
    "    a = cupy.fft.rfft(a)\n",
    "    b = cupy.fft.rfft(b)\n",
    "    a *= b.conj()\n",
    "    c = cupy.fft.irfft(a)\n",
    "    c = cupy.fft.fftshift(c)\n",
    "\n",
    "    # move array from GPU device to the host\n",
    "    return cupy.asnumpy(c)\n",
    "\n",
    "\n",
    "if not SKIP_CUDA:\n",
    "    %timeit correlate_cufft(A, B)\n",
    "    %timeit map_threaded(correlate_cufft, [A, A], [B, B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This GPU version runs slower than the CPU version due to the overhead of moving small arrays from/to the device and executing several small kernel functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Cython with a C FFT library\n",
    "\n",
    "The [fft2d C library](http://www.kurims.kyoto-u.ac.jp/~ooura/fft.html) by Takuya Ooura provides efficient functions to compute Fast Fourier Transforms (FFT).\n",
    "\n",
    "Cython makes it relatively easy to use C libraries from Python.\n",
    "\n",
    "The file **`fftsg.c` defines a function `rdft`**, which computes forward and invers DFT of real input arrays: \n",
    "\n",
    "```\n",
    "Fast Fourier/Cosine/Sine Transform\n",
    "    dimension   :one\n",
    "    data length :power of 2\n",
    "    decimation  :frequency\n",
    "    radix       :split-radix\n",
    "    data        :inplace\n",
    "    table       :use\n",
    "\n",
    "functions\n",
    "    rdft: Real Discrete Fourier Transform\n",
    "    \n",
    "function prototypes\n",
    "    void rdft(int, int, double *, int *, double *);\n",
    "\n",
    "-------- Real DFT / Inverse of Real DFT --------\n",
    "    [definition]\n",
    "        <case1> RDFT\n",
    "            R[k] = sum_j=0^n-1 a[j]*cos(2*pi*j*k/n), 0<=k<=n/2\n",
    "            I[k] = sum_j=0^n-1 a[j]*sin(2*pi*j*k/n), 0<k<n/2\n",
    "        <case2> IRDFT (excluding scale)\n",
    "            a[k] = (R[0] + R[n/2]*cos(pi*k))/2 + \n",
    "                   sum_j=1^n/2-1 R[j]*cos(2*pi*j*k/n) + \n",
    "                   sum_j=1^n/2-1 I[j]*sin(2*pi*j*k/n), 0<=k<n\n",
    "    [usage]\n",
    "        <case1>\n",
    "            ip[0] = 0; // first time only\n",
    "            rdft(n, 1, a, ip, w);\n",
    "        <case2>\n",
    "            ip[0] = 0; // first time only\n",
    "            rdft(n, -1, a, ip, w);\n",
    "    [parameters]\n",
    "        n              :data length (int)\n",
    "                        n >= 2, n = power of 2\n",
    "        a[0...n-1]     :input/output data (double *)\n",
    "                        <case1>\n",
    "                            output data\n",
    "                                a[2*k] = R[k], 0<=k<n/2\n",
    "                                a[2*k+1] = I[k], 0<k<n/2\n",
    "                                a[1] = R[n/2]\n",
    "                        <case2>\n",
    "                            input data\n",
    "                                a[2*j] = R[j], 0<=j<n/2\n",
    "                                a[2*j+1] = I[j], 0<j<n/2\n",
    "                                a[1] = R[n/2]\n",
    "        ip[0...*]      :work area for bit reversal (int *)\n",
    "                        length of ip >= 2+sqrt(n/2)\n",
    "                        strictly, \n",
    "                        length of ip >= \n",
    "                            2+(1<<(int)(log(n/2+0.5)/log(2))/2).\n",
    "                        ip[0],ip[1] are pointers of the cos/sin table.\n",
    "        w[0...n/2-1]   :cos/sin table (double *)\n",
    "                        w[],ip[] are initialized if ip[0] == 0.\n",
    "    [remark]\n",
    "        Inverse of \n",
    "            rdft(n, 1, a, ip, w);\n",
    "        is \n",
    "            rdft(n, -1, a, ip, w);\n",
    "            for (j = 0; j <= n - 1; j++) {\n",
    "                a[j] *= 2.0 / n;\n",
    "            }\n",
    "```\n",
    "\n",
    "We use Python's distutils module to compile the `fftsg.c` C code into a static link library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils import ccompiler\n",
    "\n",
    "compiler = ccompiler.new_compiler()\n",
    "objects = compiler.compile(['fft2d/fftsg.c'], extra_postargs=['-fPIC', '-O2'])\n",
    "compiler.create_static_lib(objects, 'ftt2d', output_dir='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the `ftt2d` library from Cython, we need to include the declaration from the C header file and allocate temporary arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython --compile-args=-O2 -I. -l./ftt2d\n",
    "#\n",
    "#cython: boundscheck=False\n",
    "#cython: wraparound=False\n",
    "\n",
    "import numpy\n",
    "\n",
    "from libc.stdlib cimport malloc, free\n",
    "from libc.math cimport sqrt\n",
    "\n",
    "cdef extern from 'fft2d.h':\n",
    "    void rdft(int n, int isgn, double *a, int *ip, double *w) nogil\n",
    "\n",
    "\n",
    "def correlate_cython_fft2d(a, b):\n",
    "    \"\"\"Return circular correlation of two one-dimensional arrays.\"\"\"\n",
    "    cdef:\n",
    "        ssize_t size = a.size\n",
    "        double scale = 2.0 / size\n",
    "        double[::1] a_\n",
    "        double[::1] b_\n",
    "        double *w_\n",
    "        int *ip_\n",
    "        int s\n",
    "\n",
    "    # copy input arrays. rdft computes in-place\n",
    "    result = numpy.copy(a)\n",
    "    a_ = result\n",
    "    b_ = numpy.copy(b)\n",
    "\n",
    "    with nogil:\n",
    "\n",
    "        # allocate cos/sin table\n",
    "        w_ = <double *>malloc((size // 2) * sizeof(double))\n",
    "        if not w_:\n",
    "            with gil:\n",
    "                raise MemoryError('could not allocate w_')\n",
    "\n",
    "        # allocate work area for bit reversal\n",
    "        ip_ = <int *>malloc((2 + <int>(sqrt((size//2) + 0.5))) * sizeof(int))\n",
    "        if not ip_:\n",
    "            with gil:\n",
    "                raise MemoryError('could not allocate ip_')\n",
    "        ip_[0] = 0\n",
    "\n",
    "        # forward DFT\n",
    "        rdft(size, 1, &b_[0], ip_, w_)\n",
    "        rdft(size, 1, &a_[0], ip_, w_)\n",
    "\n",
    "        # multiply by complex conjugate\n",
    "        multiply_conj(a_, b_, size)\n",
    "\n",
    "        # reverse DFT\n",
    "        rdft(size, -1, &a_[0], ip_, w_)\n",
    "\n",
    "        # shift and scale results\n",
    "        fftshift(a_, size, scale)\n",
    "\n",
    "        free(w_)\n",
    "        free(ip_)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "cdef void multiply_conj(double[::1] a, double[::1] b, ssize_t size) nogil:\n",
    "    \"\"\"In-place multiply a by complex conjugate of b.\"\"\"\n",
    "    cdef:\n",
    "        ssize_t i\n",
    "        double ar, br, ai, bi\n",
    "\n",
    "    a[0] = a[0] * b[0]\n",
    "    a[1] = a[1] * b[1]\n",
    "    for i in range(2, size, 2):\n",
    "        ar = a[i]\n",
    "        ai = a[i+1]\n",
    "        br = b[i]\n",
    "        bi = b[i+1]\n",
    "        a[i] = ar * br + ai * bi\n",
    "        a[i+1] = ai * br - ar * bi\n",
    "\n",
    "\n",
    "cdef void fftshift(double[::1] a, ssize_t size, double scale) nogil:\n",
    "    \"\"\"In-place shift zero-frequency component to center of spectrum.\"\"\"\n",
    "    cdef:\n",
    "        ssize_t i\n",
    "        double t\n",
    "\n",
    "    size //= 2\n",
    "\n",
    "    for i in range(size):\n",
    "        t = a[i]\n",
    "        a[i] = a[i + size] * scale\n",
    "        a[i + size] = t * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert numpy.allclose(correlate_cython_fft2d(A, B),\n",
    "                      correlate_fft(A, B))\n",
    "\n",
    "%timeit correlate_cython_fft2d(A, B)\n",
    "%timeit map_threaded(correlate_cython_fft2d, [A, A], [B, B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far this is the fastest implementation of the correlate function. It can be run multi-threaded, although **for short input arrays the overhead of multi-threading is detrimental**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare implementations\n",
    "\n",
    "Finally, we compare several implementations of the cross-correlate function using longer time series of 16384 samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import numpy\n",
    "from IPython.display import display\n",
    "from ipywidgets import IntProgress\n",
    "\n",
    "\n",
    "def time_functions(functions, size=2**14, max_workers=MAXCPUS):\n",
    "    \"\"\"Return runtimes of single and multi-threaded correlation functions.\"\"\"\n",
    "    progress = IntProgress(min=0, max=2*len(functions))\n",
    "    display(progress)\n",
    "\n",
    "    a = numpy.random.random(size) - 0.5\n",
    "    b = numpy.random.random(size) - 0.5\n",
    "    ab = [a]*max_workers, [b]*max_workers\n",
    "\n",
    "    result = []\n",
    "    for function in functions:\n",
    "        try:\n",
    "            func = globals()[function]\n",
    "            t0 = timeit.Timer(lambda: func(a, b)).timeit(number=1)\n",
    "            number = max(2, int(1 / t0))\n",
    "            t0 = timeit.Timer(lambda: func(a, b)).timeit(number)\n",
    "            progress.value += 1\n",
    "            t1 = timeit.Timer(lambda: map_threaded(func, *ab, \n",
    "                                                   max_workers=max_workers)\n",
    "                              ).timeit(number)\n",
    "            progress.value += 1\n",
    "            result.append(['{:.2f}'.format(t0 * 1e3 / number),\n",
    "                           '{:.2f}'.format(t1 * 1e3 / number),\n",
    "                           '{:.1f}'.format(t0/t1 * max_workers)])\n",
    "        except Exception:\n",
    "            result.append([float('nan')] * 3)\n",
    "    progress.close()\n",
    "    try:\n",
    "        import pandas\n",
    "        columns = ['1 thread / ms', '{} threads / ms'.format(max_workers),\n",
    "                   'speedup']\n",
    "        return pandas.DataFrame(result, index=functions, columns=columns)\n",
    "    except ImportError:\n",
    "        return result\n",
    "\n",
    "\n",
    "display(time_functions([\n",
    "    # 'correlate_python'\n",
    "    'correlate_numpy',\n",
    "    'correlate_cython', \n",
    "    'correlate_cython_omp',\n",
    "    'correlate_numba', \n",
    "    'correlate_numba_parallel', \n",
    "    'correlate_numba_cuda',\n",
    "    'correlate_scipy',\n",
    "    'correlate_fft', \n",
    "    'correlate_cufft',\n",
    "    'correlate_cython_fft2d']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best implementation of the correlate function is **almost as fast as desired (~200 Âµs)** for the pCF analysis of images.\n",
    "\n",
    "The correlate function could be further optimized by implementing it in C++ and using the `DFTi` functions of the closed source [Intel MKL](https://software.intel.com/en-us/intel-mkl) library. Expect another 30% speed improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Implement pair correlation function analysis of small image time series\n",
    "\n",
    "Now that we have developed a fast cross-correlation function and learned techniques to speed-up Python code, we use them to analyze a small simulated time series of images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and explore simulated images\n",
    "\n",
    "The **`Simulation_Channel.bin`** file contains the result of a simulation of fluorescent particles diffusing on a **64x64 grid**. The grid contains a **diagonal, 300 nm wide channel**, which restricts free diffusion. The file was produced using the  [Globals for Images Â· SimFCS](http://www.lfd.uci.edu/globals/) software.\n",
    "\n",
    "The simulated images at **32,000 time steps** are stored contiguously as **16-bit unsigned integers** in the file. \n",
    "\n",
    "The time samples are not stored contiguously. Accessing time series will be inefficient while spatial access will be fast:\n",
    "\n",
    "![time_vs_spatial_access](time_vs_spatial_access.png)\n",
    "\n",
    "The [`numpy.fromfile`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.fromfile.html) function can be used to load the raw binary data into a 3D numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "\n",
    "def rawread(filename, shape, dtype):\n",
    "    \"\"\"Return array data from binary file.\"\"\"\n",
    "    count = numpy.prod(shape, dtype='intp')\n",
    "    count = count if count >= 0 else -1\n",
    "    data = numpy.fromfile(filename, dtype=dtype, count=count)\n",
    "    data.shape = shape\n",
    "    return data\n",
    "\n",
    "\n",
    "SIMULATION_DATA = rawread(os.path.join(DATA_PATH, 'Simulation_Channel.bin'), \n",
    "                          shape=(-1, 64, 64), dtype='uint16')\n",
    "\n",
    "print('Data shape:', SIMULATION_DATA.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For FFTs to be performed efficiently, the **size of the time axis is truncated to a power of two**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape2pow2(data, axis):\n",
    "    \"\"\"Return array with axis truncated to power of 2.\"\"\"\n",
    "    try:\n",
    "        iter(axis)\n",
    "    except TypeError:\n",
    "        axis = [axis]\n",
    "    slices = []\n",
    "    for i, size in enumerate(data.shape):\n",
    "        if i in axis:\n",
    "            size = 2**int(math.log(size, 2))\n",
    "        slices.append(slice(0, size))\n",
    "    return data[slices]\n",
    "\n",
    "\n",
    "SIMULATION_DATA = shape2pow2(SIMULATION_DATA, axis=0)\n",
    "\n",
    "print('Truncated shape:', SIMULATION_DATA.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the averages over the time axis (mean image) and the spatial axes (mean time series):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "def plot_image_timeseries(image_timeseries):\n",
    "    \"\"\"Plot temporal and spatial means of image timeseries.\"\"\"\n",
    "    pyplot.figure(figsize=(6, 8))\n",
    "    \n",
    "    pyplot.subplot(3, 1, (1, 2))\n",
    "    mean_image = numpy.mean(image_timeseries, axis=0)\n",
    "    pyplot.title('image of temporal mean')\n",
    "    pyplot.imshow(mean_image, cmap='viridis', interpolation='none')\n",
    "    pyplot.colorbar(shrink=0.83, pad=0.05)\n",
    "    \n",
    "    pyplot.subplot(3, 1, 3)\n",
    "    mean_ts = numpy.mean(image_timeseries, axis=(1, 2))\n",
    "    pyplot.title('time series of spatial mean')\n",
    "    pyplot.xlabel('time index')\n",
    "    pyplot.ylabel('intensity')\n",
    "    pyplot.plot(mean_ts)\n",
    "    pyplot.xlim([0, len(mean_ts)])\n",
    "    \n",
    "    pyplot.tight_layout()\n",
    "    pyplot.show()\n",
    "\n",
    "\n",
    "plot_image_timeseries(SIMULATION_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process cross-correlation functions for image fluorescence fluctuation analysis\n",
    "\n",
    "In image fluctuation correlation spectroscopy, the cross-correlation of two time series of fluorescence intensity, $F_a(t)$ and $F_b(t)$, are regularly processed and presented as follows:\n",
    "\n",
    "1. The correlation functions are **normalized** as follows:\n",
    "   \n",
    "   $$ G(\\tau) = \\dfrac{\\big \\langle F_a(t) \\cdot F_b(t+\\tau) \\big \\rangle}{\\langle F_a(t) \\rangle \\cdot \\langle F_b(t) \\rangle} - 1  =  \\dfrac{\\big \\langle \\big(F_a(t) - \\langle F_a(t) \\rangle\\big) \\cdot \\big(F_b(t) - \\langle F_b(t) \\rangle\\big) \\big \\rangle}{\\langle F_a(t) \\rangle \\cdot \\langle F_b(t) \\rangle},$$\n",
    "   \n",
    "   where $F(t)$ is the fluorescence intensity signal at time $t$, $\\tau$ is the time delay, and $\\ \\langle \\rangle$ the mean.\n",
    "\n",
    "2. Only the **positive time delays $\\tau$** are used. This corresponds to the negative delays for the definition of cross-correlation we used.\n",
    "\n",
    "3. The functions are **averaged in exponentially increasing bins of time delays**.\n",
    "\n",
    "4. The log-binned functions are **smoothed**.\n",
    "\n",
    "We define the following functions for fluctuation correlation analysis of time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "\n",
    "def correlate_circular(a, b):\n",
    "    \"\"\"Return circular correlation of two arrays using DFT.\"\"\"\n",
    "    size = a.size\n",
    "\n",
    "    # forward DFT\n",
    "    a = numpy.fft.rfft(a)\n",
    "    b = numpy.fft.rfft(b)\n",
    "    # multiply by complex conjugate\n",
    "    c = a.conj() * b\n",
    "    # reverse DFT\n",
    "    c = numpy.fft.irfft(c)\n",
    "    \n",
    "    # positive delays only\n",
    "    c = c[:size // 2]\n",
    "        \n",
    "    # normalize with the averages of a and b\n",
    "    #   c is already normalized by size\n",
    "    #   the 0th value of the DFT contains the sum of the signal\n",
    "    c /= a[0].real * b[0].real / size\n",
    "    c -= 1.0\n",
    "    \n",
    "    return c\n",
    "\n",
    "\n",
    "def correlate_linear(a, b):\n",
    "    \"\"\"Return linear correlation of two arrays using DFT.\"\"\"\n",
    "    size = a.size\n",
    "    \n",
    "    # subtract mean and pad with zeros to twice the size\n",
    "    a_mean = a.mean()\n",
    "    b_mean = b.mean()\n",
    "    a = numpy.pad(a-a_mean, a.size//2, mode='constant')\n",
    "    b = numpy.pad(b-b_mean, b.size//2, mode='constant')\n",
    "\n",
    "    # forward DFT\n",
    "    a = numpy.fft.rfft(a)\n",
    "    b = numpy.fft.rfft(b)\n",
    "    # multiply by complex conjugate\n",
    "    c = a.conj() * b\n",
    "    # reverse DFT\n",
    "    c = numpy.fft.irfft(c)\n",
    "    # positive delays only\n",
    "    c = c[:size // 2]\n",
    "        \n",
    "    # normalize with the averages of a and b\n",
    "    c /= size * a_mean * b_mean\n",
    "    \n",
    "    return c\n",
    "\n",
    "\n",
    "def average(c, bins):\n",
    "    \"\"\"Return averaged chunks of array.\"\"\"\n",
    "    out = [numpy.mean(c[:bins[0]])]\n",
    "    for i in range(len(bins)-1):\n",
    "        out.append(numpy.mean(c[bins[i]:bins[i+1]]))\n",
    "    return out\n",
    "\n",
    "\n",
    "def logbins(size, nbins):\n",
    "    \"\"\"Return up to nbins exponentially increasing integers from 1 to size.\"\"\"\n",
    "    b = numpy.logspace(0, math.log(size, 2), nbins, base=2, endpoint=True)\n",
    "    return numpy.unique(b.astype('intp'))\n",
    "\n",
    "\n",
    "def smooth(c):\n",
    "    \"\"\"Return double exponentially smoothed array.\"\"\"\n",
    "    out = c.copy()\n",
    "    out[0] = out[1]\n",
    "    for i in range(1, len(out)):\n",
    "        out[i] = out[i] * 0.3 + out[i-1] * 0.7\n",
    "    for i in range(len(out)-2, -1, -1):\n",
    "        out[i] = out[i] * 0.3 + out[i+1] * 0.7\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot two time series, their normalized linear and circular cross-correlation functions, the log-binned functions, and the final **smoothed log-binned normalized cross-correlation functions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "\n",
    "def plot_pcf_processing(image_timeseries):\n",
    "    \"\"\"Compare linear and circular pair correlation functions.\"\"\"\n",
    "    ntimes, height, width = image_timeseries.shape\n",
    "\n",
    "    def _plot(y0, x0, y1, x1):\n",
    "        # select time series from image_timeseries\n",
    "        a = image_timeseries[:, y0, x0]\n",
    "        b = image_timeseries[:, y1, x1]\n",
    "\n",
    "        # linear and circular correlation\n",
    "        cl = correlate_linear(a, b)\n",
    "        cc = correlate_circular(a, b)\n",
    "\n",
    "        # average and smooth\n",
    "        bins = logbins(a.size//2, 32)\n",
    "        averagedl = average(cl, bins)\n",
    "        smoothedl = smooth(averagedl)\n",
    "        averagedc = average(cc, bins)\n",
    "        smoothedc = smooth(averagedc)\n",
    "\n",
    "        pyplot.figure(figsize=(6, 12))\n",
    "\n",
    "        # plot the time series\n",
    "        pyplot.subplot(4, 1, 1)\n",
    "        pyplot.title('time series')\n",
    "        pyplot.xlabel('time index')\n",
    "        pyplot.ylabel('intensity')\n",
    "        pyplot.plot(a, 'g', label='[{}, {}]'.format(y0, x0))\n",
    "        pyplot.plot(b, 'b', label='[{}, {}]'.format(y1, x0))\n",
    "        pyplot.xlim([0, len(a)])\n",
    "        pyplot.legend(fancybox=True, framealpha=0.9)\n",
    "\n",
    "        # plot the cross-correlation function and logbins\n",
    "        pyplot.subplot(4, 1, 2)\n",
    "        pyplot.title('normalized cross-correlation functions and logbins')\n",
    "        pyplot.xlabel('positive time delay index')\n",
    "        pyplot.ylabel('correlation')\n",
    "        for x in bins:\n",
    "            pyplot.axvline(x=x, color='0.8')\n",
    "        pyplot.plot(cl, 'g', label='linear')\n",
    "        pyplot.plot(cc, 'b', label='circular')\n",
    "        pyplot.xlim([0, len(cc)])\n",
    "        pyplot.legend(fancybox=True, framealpha=0.9)\n",
    "\n",
    "        # log-plot the cross-correlation function and logbins\n",
    "        pyplot.subplot(4, 1, 3)\n",
    "        pyplot.title('log-plot of cross-correlation functions and logbins')\n",
    "        pyplot.xlabel('positive time delay index')\n",
    "        pyplot.ylabel('correlation')\n",
    "        for x in bins:\n",
    "            pyplot.axvline(x=x, color='0.8')\n",
    "        pyplot.semilogx(cl, 'g', label='linear', basex=2)\n",
    "        pyplot.semilogx(cc, 'b', label='circular', basex=2)\n",
    "        pyplot.xlim([0, len(cc)])\n",
    "        pyplot.legend(fancybox=True, framealpha=0.9)\n",
    "\n",
    "        # plot the binned and smoothed cross-correlation function\n",
    "        pyplot.subplot(4, 1, 4)\n",
    "        pyplot.title('averaged and smoothed cross-correlation functions')\n",
    "        pyplot.xlabel('positive log time delay index')\n",
    "        pyplot.ylabel('correlation')\n",
    "        pyplot.plot(averagedl, 'g', label='linear')\n",
    "        pyplot.plot(smoothedl, 'm')\n",
    "        pyplot.plot(averagedc, 'b', label='circular')\n",
    "        pyplot.plot(smoothedc, 'r', label='smoothed')\n",
    "        pyplot.legend(fancybox=True, framealpha=0.9)\n",
    "\n",
    "        pyplot.tight_layout()\n",
    "        pyplot.show()\n",
    "\n",
    "    interact(_plot,\n",
    "             y0=IntSlider(31, 0, height-1, continuous_update=False),\n",
    "             x0=IntSlider(31, 0, width-1, continuous_update=False),\n",
    "             y1=IntSlider(35, 0, height-1, continuous_update=False),\n",
    "             x1=IntSlider(35, 0, width-1, continuous_update=False))\n",
    "\n",
    "\n",
    "plot_pcf_processing(SIMULATION_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the cross-correlation curves differ significantly at larger delays, when averaged into log-bins the differences are minimal and not significant for our application. Hence, we continue using the faster circular correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference implementation of pair correlation image analysis\n",
    "\n",
    "We are ready to implement and run the pair correlation function analysis on small images.\n",
    "\n",
    "For the reference implementation of the `ipcf` function, we use the previously defined pseudo code and the circular correlate function using numpy.fft:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from numpy import zeros\n",
    "\n",
    "\n",
    "def ipcf_reference(image_timeseries, circle_coordinates, bins):\n",
    "    \"\"\"Return pair correlation function analysis of image time series.\n",
    "\n",
    "    Cross-correlate the time series of each pixel in the image\n",
    "    with all its neighbors at a certain radius and return all\n",
    "    the log-binned and smoothed correlation curves.\n",
    "\n",
    "    \"\"\"\n",
    "    ntimes, height, width = image_timeseries.shape\n",
    "    npoints = len(circle_coordinates)\n",
    "    radius = circle_coordinates[0, 0]\n",
    "\n",
    "    result = zeros((height-2*radius, width-2*radius, npoints, len(bins)),\n",
    "                   'float32')\n",
    "\n",
    "    for y in range(radius, height-radius):\n",
    "        for x in range(radius, width-radius):\n",
    "            a = image_timeseries[:, y, x]\n",
    "            for i in range(npoints):\n",
    "                u, v = circle_coordinates[i]\n",
    "                b = image_timeseries[:, y+v, x+u]\n",
    "                c = correlate(b, a)\n",
    "                result[y-radius, x-radius, i] = smooth(average(c, bins))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def correlate(a, b):\n",
    "    \"\"\"Return normalized circular correlation using DFT.\"\"\"\n",
    "    size = a.size\n",
    "    # forward DFT\n",
    "    a = numpy.fft.rfft(a)\n",
    "    b = numpy.fft.rfft(b)\n",
    "    # multiply by complex conjugate\n",
    "    c = a * b.conj()\n",
    "    # reverse DFT\n",
    "    c = numpy.fft.irfft(c)\n",
    "    # positive delays only\n",
    "    c = c[:size // 2]\n",
    "    # normalize with the averages of a and b\n",
    "    #   c is already normalized by size\n",
    "    #   the 0th value of the DFT contains the sum of the signal\n",
    "    c /= a[0].real * b[0].real / size\n",
    "    c -= 1.0\n",
    "    return c\n",
    "\n",
    "\n",
    "def average(c, bins):\n",
    "    \"\"\"Return averaged chunks of array.\"\"\"\n",
    "    out = [numpy.mean(c[:bins[0]])]\n",
    "    for i in range(len(bins)-1):\n",
    "        out.append(numpy.mean(c[bins[i]:bins[i+1]]))\n",
    "    return out\n",
    "\n",
    "\n",
    "def smooth(c):\n",
    "    \"\"\"Return double exponentially smoothed array.\"\"\"\n",
    "    out = c.copy()\n",
    "    out[0] = out[1]\n",
    "    for i in range(1, len(out)):\n",
    "        out[i] = out[i] * 0.3 + out[i-1] * 0.7\n",
    "    for i in range(len(out)-2, -1, -1):\n",
    "        out[i] = out[i] * 0.3 + out[i+1] * 0.7\n",
    "    return out\n",
    "\n",
    "\n",
    "def logbins(size, nbins):\n",
    "    \"\"\"Return up to nbins exponentially increasing integers from 1 to size.\"\"\"\n",
    "    b = numpy.logspace(0, math.log(size, 2), nbins, base=2, endpoint=True)\n",
    "    return numpy.unique(b.astype('intp'))\n",
    "\n",
    "\n",
    "def circle(radius, npoints):\n",
    "    \"\"\"Return cartesian coordinates of circle on integer grid.\"\"\"\n",
    "    angles = numpy.linspace(0, 2*numpy.pi, npoints, endpoint=False)\n",
    "    coordinates = radius * numpy.array((numpy.cos(angles), numpy.sin(angles)))\n",
    "    return numpy.ascontiguousarray(numpy.round(coordinates).T.astype('intp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all functions are defined, we can analyze the simulated data and compare it to the know results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "\n",
    "def run_ipcf(ipcf_function, image_timeseries, radius=6, npoints=32, nbins=32,\n",
    "             **kwargs):\n",
    "    \"\"\"Run ipcf_function on image_timeseries.\"\"\"\n",
    "    ntimes, height, width = image_timeseries.shape\n",
    "   \n",
    "    # truncate time axis to power of two\n",
    "    ntimes = 2**int(math.log(ntimes, 2))\n",
    "    image_timeseries = image_timeseries[:ntimes]\n",
    "    \n",
    "    # calculate circle coordinates\n",
    "    circle_coordinates = circle(radius, npoints)\n",
    "    \n",
    "    # calculate log-bins\n",
    "    bins = logbins(ntimes // 2, nbins)\n",
    "  \n",
    "    # run the pair correlation function analysis\n",
    "    result = ipcf_function(image_timeseries, circle_coordinates, bins,\n",
    "                           **kwargs)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def test_ipcf(result, expected=None, atol=1e-6):\n",
    "    \"\"\"Compare ipcf result to known results from file.\"\"\"\n",
    "    if expected is None:\n",
    "        expected = SIMULATION_IPCF_EXPECTED\n",
    "    if not numpy.allclose(result, expected, atol=atol):\n",
    "        try:\n",
    "            plot_ipcf_results(result - expected)\n",
    "        except NameError:\n",
    "            print('Test failed')\n",
    "\n",
    "\n",
    "SIMULATION_IPCF_EXPECTED = rawread(\n",
    "    os.path.join(DATA_PATH, 'Simulation_Channel.ipcf.bin'), \n",
    "    (52, 52, 32, 30), 'float32')\n",
    "\n",
    "%time SIMULATION_IPCF_RESULT = run_ipcf(ipcf_reference, SIMULATION_DATA)\n",
    "\n",
    "test_ipcf(SIMULATION_IPCF_RESULT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About two minutes to analyze a small simulated dataset is slower than expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results of image pair correlation function analysis\n",
    "\n",
    "There are two meaningful ways to plot the 4-dimensional array returned by the ipCF analysis.\n",
    "\n",
    "First, we plot all the pair correlation curves for a selected pixel (aka sprites):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "from ipywidgets import interact, IntSlider, Dropdown\n",
    "\n",
    "\n",
    "def plot_ipcf_sprites(ipcf_result, figsize=(6, 5)):\n",
    "    \"\"\"Interactively plot pair correlation functions at pixel.\"\"\"\n",
    "    height, width, npoints, nbins = ipcf_result.shape\n",
    "\n",
    "    # data limits\n",
    "    vmax, vmin = numpy.max(ipcf_result), numpy.min(ipcf_result)\n",
    "    vminmax = max(abs(vmax), abs(vmin))\n",
    "\n",
    "    # coordinates for polar plot and Delaunay triangulation\n",
    "    radius = numpy.arange(nbins)\n",
    "    angles = numpy.linspace(0, 2*numpy.pi, npoints, endpoint=False)\n",
    "    radius, angles = numpy.meshgrid(radius, angles)\n",
    "    xcoords = radius * numpy.cos(angles)\n",
    "    ycoords = radius * numpy.sin(-angles)\n",
    "\n",
    "    def _plot(style, y, x):\n",
    "        pyplot.figure(figsize=figsize)\n",
    "        pyplot.title('pair correlation functions at pixel')\n",
    "        sprite = ipcf_result[y, x]\n",
    "        if style == 'lines':\n",
    "            pyplot.plot(sprite.T, 'b')\n",
    "            pyplot.ylim([vmin, vmax])\n",
    "            pyplot.xlabel('log time delay index')\n",
    "            pyplot.ylabel('pcf')\n",
    "        elif style == 'carpet':\n",
    "            pyplot.imshow(sprite, vmin=-vminmax, vmax=vminmax, \n",
    "                          cmap='seismic', interpolation='none')\n",
    "            pyplot.xlabel('log time delay index')\n",
    "            pyplot.ylabel('circle point index')\n",
    "            pyplot.colorbar()\n",
    "        elif style == 'polar':\n",
    "            # polar plot using Delaunay triangulation\n",
    "            pyplot.tripcolor(xcoords.flat, ycoords.flat, sprite.flat, \n",
    "                             vmin=-vminmax, vmax=vminmax,\n",
    "                             shading='gouraud', cmap='seismic')\n",
    "            pyplot.axes().set_aspect('equal')\n",
    "            pyplot.axis('off')\n",
    "            pyplot.colorbar()\n",
    "        pyplot.show()\n",
    "\n",
    "    interact(_plot, \n",
    "             style=Dropdown(options=['carpet', 'polar', 'lines']),\n",
    "             y=IntSlider(height//2, 0, height-1, continuous_update=False), \n",
    "             x=IntSlider(width//2, 0, width-1, continuous_update=False))\n",
    "\n",
    "\n",
    "plot_ipcf_sprites(SIMULATION_IPCF_RESULT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we plot the image of all pair correlation values at a selected circle point and bin index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "\n",
    "def plot_ipcf_images(ipcf_result, figsize=(6, 5), interpolation='none'):\n",
    "    \"\"\"Interactively plot image of pair correlation function values.\"\"\"\n",
    "    height, width, npoints, nbins = ipcf_result.shape\n",
    "    transpose = height > 1.5 * width\n",
    "\n",
    "    # data limits\n",
    "    vmax, vmin = numpy.max(ipcf_result), numpy.min(ipcf_result)\n",
    "    vminmax = max(abs(vmax), abs(vmin))\n",
    "\n",
    "    def _plot(point, bin):\n",
    "        pyplot.figure(figsize=figsize)\n",
    "        image = ipcf_result[:, :, point, bin]\n",
    "        if transpose:\n",
    "            image = image.T\n",
    "        angle = 360.0 / npoints * point\n",
    "        pyplot.title('pair correlation function values')\n",
    "        pyplot.imshow(image, vmin=-vminmax, vmax=vminmax, \n",
    "                      cmap='seismic', interpolation=interpolation)\n",
    "        orientation = 'horizontal' if transpose else 'vertical'\n",
    "        pyplot.colorbar(orientation=orientation)\n",
    "        pyplot.show()\n",
    "\n",
    "    interact(_plot, \n",
    "             point=IntSlider(npoints//2, 0, npoints-1, \n",
    "                             continuous_update=False), \n",
    "             bin=IntSlider(nbins//2, 0, nbins-1, \n",
    "                           continuous_update=False))\n",
    "\n",
    "\n",
    "plot_ipcf_images(SIMULATION_IPCF_RESULT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Optimize the `ipcf` function\n",
    "\n",
    "The algorithm and implementation of the image pair correlation function can be improved by the following means:\n",
    "\n",
    "* change the data layout such that the **time axis becomes contiguous** and the individual time series can be accessed much faster\n",
    "\n",
    "* **pre-calculate the forward DFT** and use it for cross-correlation instead of the data. For our dataset, this will take additional 512 MB RAM\n",
    "\n",
    "* redefine functions to **not allocate new arrays on every function call** but to write directly to the output array\n",
    "\n",
    "* use the **symmetry of the cross-correlation function** `correlate(a, b) == correlate(b, a)[::-1]` to avoid duplicate calculations\n",
    "\n",
    "* decorate the functions with numba.jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import numba\n",
    "\n",
    "\n",
    "def ipcf_optimized(image_timeseries, circle_coordinates, bins, **kwargs):\n",
    "    \"\"\"Return pair correlation function analysis of image time series.\"\"\"\n",
    "    ntimes, height, width = image_timeseries.shape\n",
    "    npoints = len(circle_coordinates)\n",
    "    nbins = len(bins)\n",
    "    radius = circle_coordinates[0, 0]\n",
    "\n",
    "    result = numpy.zeros((height-2*radius, width-2*radius, npoints, nbins),\n",
    "                         'float32')\n",
    "\n",
    "    # make time axis last dimension\n",
    "    data = numpy.moveaxis(image_timeseries, 0, -1)\n",
    "\n",
    "    # pre-calculate forward DFT along time axis\n",
    "    rfft_buffer = numpy.fft.rfft(data, axis=-1)\n",
    "\n",
    "    for y in range(radius, height-radius):\n",
    "        for x in range(radius, width-radius):\n",
    "\n",
    "            rfft_a = rfft_buffer[y, x].conj()\n",
    "\n",
    "            for i in range(npoints):\n",
    "\n",
    "                # continue if output was already calculated\n",
    "                if result[y-radius, x-radius, i, 0] != 0.0:\n",
    "                    continue\n",
    "\n",
    "                u, v = circle_coordinates[i]\n",
    "                rfft_b = rfft_buffer[y+v, x+u]\n",
    "\n",
    "                # cross-correlate b and a\n",
    "                c = numpy.fft.irfft(rfft_a * rfft_b)\n",
    "\n",
    "                scale = ntimes / rfft_a[0].real / rfft_b[0].real\n",
    "\n",
    "                # positive delays\n",
    "                average_smooth_scale(c, bins, scale,\n",
    "                                     result[y-radius, x-radius, i])\n",
    "\n",
    "                # negative delays\n",
    "                if (radius <= y+v < height-radius and\n",
    "                    radius <= x+u < width-radius):\n",
    "                    c = numpy.fft.fftshift(c)\n",
    "                    i = (i + npoints // 2) % npoints\n",
    "                    average_smooth_scale(c[ntimes//2:0:-1], bins, scale,\n",
    "                                         result[y+v-radius, x+u-radius, i])\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "@numba.jit(nogil=True)\n",
    "def average_smooth_scale(c, bins, scale, out):\n",
    "    \"\"\"Average, smooth, and scale correlation function.\"\"\"\n",
    "    # average\n",
    "    out[0] = numpy.mean(c[:bins[0]])\n",
    "    for i in range(len(bins)-1):\n",
    "        out[i+1] = numpy.mean(c[bins[i]:bins[i+1]])\n",
    "\n",
    "    # smooth\n",
    "    out[0] = out[1]\n",
    "    for i in range(1, len(bins)):\n",
    "        out[i] = out[i] * 0.3 + out[i-1] * 0.7\n",
    "    for i in range(len(bins)-2, -1, -1):\n",
    "        out[i] = out[i] * 0.3 + out[i+1] * 0.7\n",
    "\n",
    "    # scale\n",
    "    out *= scale\n",
    "    out -= 1.0\n",
    "\n",
    "\n",
    "%time ipcf_result = run_ipcf(ipcf_optimized, SIMULATION_DATA)\n",
    "\n",
    "test_ipcf(ipcf_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's **about 6 times faster than the reference implementation**. This might be good enough to analyze larger datasets in parallel, however this implementation does not multi-thread very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A fast `ipcf` function using Cython, OpenMP, and fft2d\n",
    "\n",
    "Now that we have optimized the algorithm of the pCF analysis of images, let's put it all together and implement the function in Cython using OpenMP and the fft2d C library, which was compiled to a static library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython --compile-args=-O2 -I. -l./ftt2d  $OPENMP_ARGS\n",
    "#\n",
    "#cython: boundscheck=False\n",
    "#cython: wraparound=False\n",
    "#cython: cdivision=True\n",
    "\n",
    "import math\n",
    "import numpy\n",
    "from cython.parallel import prange, parallel\n",
    "\n",
    "cimport numpy\n",
    "from libc.stdlib cimport malloc, free\n",
    "from libc.math cimport sqrt\n",
    "\n",
    "cdef extern from 'fft2d.h':\n",
    "    void rdft(int n, int isgn, double *a, int *ip, double *w) nogil\n",
    "\n",
    "\n",
    "def ipcf_cython(numpy.uint16_t [:, :, :] image_timeseries not None,\n",
    "                ssize_t [:, ::1] circle_coordinates not None,\n",
    "                ssize_t [::1] bins not None,\n",
    "                int num_threads=0):\n",
    "    \"\"\"Return pair correlation function analysis of image time series.\"\"\"\n",
    "    cdef:\n",
    "        ssize_t ntimes = image_timeseries.shape[0]\n",
    "        ssize_t height = image_timeseries.shape[1]\n",
    "        ssize_t width = image_timeseries.shape[2]\n",
    "        ssize_t nbins = bins.shape[0]\n",
    "        ssize_t npoints = circle_coordinates.shape[0]\n",
    "        ssize_t radius = circle_coordinates[0, 0]\n",
    "        ssize_t x, y, u, v, i, t, x1, y1, t1\n",
    "        double scale\n",
    "        double *rfft_a\n",
    "        double *rfft_b\n",
    "        double *a_\n",
    "        double *w_\n",
    "        int *ip_\n",
    "        double [:, :, ::1] rdft_\n",
    "        float[:, :, :, ::1] out\n",
    "        \n",
    "    # limit length of time axis to power of two\n",
    "    ntimes = 2**int(math.log(ntimes, 2))\n",
    "\n",
    "    if radius < 2:\n",
    "        raise ValueError('invalid radius')\n",
    "    if width <= 2*radius or height <= 2*radius:\n",
    "        raise ValueError('invalid image size')\n",
    "    if ntimes < 32 or ntimes > 2147483647:\n",
    "        raise ValueError('invalid size of time axis')\n",
    "\n",
    "    # output array\n",
    "    result = numpy.zeros((height-2*radius, width-2*radius, npoints, nbins),\n",
    "                         dtype='float32')\n",
    "    out = result\n",
    "\n",
    "    # buffer for forward DFT\n",
    "    rdft_ = numpy.empty((height, width, ntimes), dtype='float64')\n",
    "\n",
    "    with nogil:\n",
    "        # rdft cos/sin table\n",
    "        w_ = <double *>malloc(ntimes // 2 * sizeof(double))\n",
    "        if not w_:\n",
    "            with gil:\n",
    "                raise MemoryError('could not allocate w_')\n",
    "\n",
    "        # rdft work area for bit reversal\n",
    "        ip_ = <int*>malloc((2 + <int>(sqrt((ntimes//2) + 0.5))) * sizeof(int))\n",
    "        if not ip_:\n",
    "            with gil:\n",
    "                raise MemoryError('could not allocate ip_')\n",
    "\n",
    "        # initialize ip_ and w_\n",
    "        ip_[0] = 0\n",
    "        rdft(ntimes, 1, &rdft_[0, 0, 0], ip_, w_)\n",
    "\n",
    "\n",
    "    with nogil, parallel(num_threads=num_threads):\n",
    "        # thread-local input/output data\n",
    "        a_ = <double *>malloc(sizeof(double) * ntimes)\n",
    "        if not a_:\n",
    "            with gil:\n",
    "                raise MemoryError('could not allocate a_')\n",
    "\n",
    "        # forward DFT\n",
    "        for y1 in prange(height):\n",
    "            for x1 in range(width):\n",
    "                for t1 in range(ntimes):\n",
    "                    rdft_[y1, x1, t1] = <double>image_timeseries[t1, y1, x1]\n",
    "                rdft(ntimes, 1, &rdft_[y1, x1, 0], ip_, w_)\n",
    "\n",
    "        # cross-correlation\n",
    "        for y in prange(radius, height-radius):\n",
    "            for x in range(radius, width-radius):\n",
    "                rfft_a = &rdft_[y, x, 0]\n",
    "\n",
    "                for i in range(npoints):\n",
    "                    # continue if output was already calculated\n",
    "                    if out[y-radius, x-radius, i, 0] != 0.0:\n",
    "                        continue\n",
    "\n",
    "                    u = x + circle_coordinates[i, 0]\n",
    "                    v = y + circle_coordinates[i, 1]\n",
    "\n",
    "                    rfft_b = &rdft_[v, u, 0]\n",
    "\n",
    "                    # multiply b's DFT by complex conjugate of a's DFT\n",
    "                    multiply_conj(rfft_b, rfft_a, a_, ntimes)\n",
    "\n",
    "                    # invers DFT\n",
    "                    rdft(ntimes, -1, a_, ip_, w_)\n",
    "\n",
    "                    scale = 2.0 / (rfft_a[0] * rfft_b[0])\n",
    "\n",
    "                    # positive delays\n",
    "                    average_smooth_scale(a_, ntimes, bins, nbins, scale,\n",
    "                                         out[y-radius, x-radius, i])\n",
    "\n",
    "                    # negative delays\n",
    "                    if ((v >= radius) and (v < height-radius) and\n",
    "                        (u >= radius) and (u < width-radius)):\n",
    "                        i = (i + npoints // 2) % npoints\n",
    "                        average_smooth_scale(a_, ntimes, bins, nbins, scale,\n",
    "                                             out[v-radius, u-radius, i], -1)\n",
    "        free(a_)\n",
    "\n",
    "    free(w_)\n",
    "    free(ip_)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "cdef void multiply_conj(double *a, double *b, double *c, ssize_t size) nogil:\n",
    "    \"\"\"Multiply `a` by complex conjugate of `b` and store in `c`.\"\"\"\n",
    "    cdef ssize_t i\n",
    "    cdef double ar, br, ai, bi\n",
    "\n",
    "    c[0] = a[0] * b[0]\n",
    "    c[1] = a[1] * b[1]\n",
    "    for i in range(2, size, 2):\n",
    "        ar = a[i]\n",
    "        ai = a[i+1]\n",
    "        br = b[i]\n",
    "        bi = b[i+1]\n",
    "        c[i] = ar * br + ai * bi\n",
    "        c[i+1] = ai * br - ar * bi\n",
    "\n",
    "\n",
    "cdef void average_smooth_scale(double *a, ssize_t size,\n",
    "                               ssize_t[::1] bins, ssize_t nbins, double scale,\n",
    "                               float[::1] out, int mode=1) nogil:\n",
    "    \"\"\"Average, smooth, and scale correlation function.\n",
    "\n",
    "    The first nbins items of the input array are changed.\n",
    "\n",
    "    \"\"\"\n",
    "    cdef ssize_t i, j\n",
    "    cdef double s\n",
    "\n",
    "    # average\n",
    "    if mode == 1:\n",
    "        # positive delays\n",
    "        s = 0.0\n",
    "        for i in range(bins[0]):\n",
    "            s += a[i]\n",
    "        a[0] = s / <double>bins[0]\n",
    "        for j in range(1, nbins):\n",
    "            s = 0.0\n",
    "            for i in range(bins[j-1], bins[j]):\n",
    "                s += a[i]\n",
    "            a[j] = s / <double>(bins[j] - bins[j-1])\n",
    "    else:\n",
    "        # negative delay\n",
    "        s = a[0]\n",
    "        for i in range(1, bins[0]):\n",
    "            s += a[size - i]\n",
    "        a[0] = <float>(s / <double>bins[0])\n",
    "        for j in range(1, nbins):\n",
    "            s = 0.0\n",
    "            for i in range(bins[j-1], bins[j]):\n",
    "                s += a[size - i]\n",
    "            a[j] = s / <double>(bins[j] - bins[j-1])\n",
    "\n",
    "    # smooth\n",
    "    a[0] = a[1]\n",
    "    for i in range(1, nbins):\n",
    "        a[i] = a[i] * 0.3 + a[i-1] * 0.7\n",
    "    for i in range(nbins-2, -1, -1):\n",
    "        a[i] = a[i] * 0.3 + a[i+1] * 0.7\n",
    "\n",
    "    # copy to output with scaling\n",
    "    for i in range(nbins):\n",
    "        out[i] = <float>(a[i] * scale - 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time ipcf_result = run_ipcf(ipcf_cython, SIMULATION_DATA, num_threads=MAXCPUS)\n",
    "\n",
    "test_ipcf(ipcf_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is **about 80 times faster than the initial implementation in Python** using numpy.fft and fast enough for the analysis of big image time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Implement out-of-core pair correlation function analysis of big image time series\n",
    "\n",
    "In this section, we use the fast image pair correlation function on small, overlapping chunks of a big image time series that is too large to fit into the computer's main memory at once. \n",
    "\n",
    "First, we need to convert the experimental data files into a more efficient format, remove large areas of background, and correct the data for photobleaching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Browse SPIM time series of images\n",
    "\n",
    "The directory `nih3t3-egfp_2` contains a **34.5 GB dataset of 35,000 TIFF files**. Each file contains a single 1024x512 pixel image stored as 16-bit unsigned integers in big-endian byte order:\n",
    "\n",
    "![image timeseries](image_timeseries.png)\n",
    "\n",
    "The files were acquired as a 2D image time series using ÂµManager software and a custom-built Selective Plane Illumination Microscopy (SPIM) instrument at the Laboratory for Fluorescence Dynamics, University of California, Irvine.\n",
    "\n",
    "The sample is a NIH3T3 cells expressing EGFP, imaged with a pixel size of 76 nm at 83 frames per seconds.\n",
    "\n",
    "Let's interactively browse the images in the time series using the [tifffile.py](http://www.lfd.uci.edu/~gohlke/code/tifffile.py.html) module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "import tifffile\n",
    "\n",
    "\n",
    "def browse_images(filenames, vmin=0, vmax=None, imread=tifffile.imread):\n",
    "    \"\"\"Interactively plot series of image files.\"\"\"\n",
    "    if not filenames:\n",
    "        raise ValueError('data files not found')\n",
    "\n",
    "    def _plot(fileindex=0):\n",
    "        filename = filenames[fileindex]\n",
    "        image = imread(filename)\n",
    "        pyplot.figure(figsize=(8, 6))\n",
    "        pyplot.title(os.path.split(filename)[-1])\n",
    "        pyplot.imshow(image.T, vmin=vmin, vmax=vmax, cmap='viridis',\n",
    "                      interpolation='lanczos')\n",
    "        pyplot.colorbar(orientation='horizontal')\n",
    "        pyplot.show()\n",
    "\n",
    "    interact(_plot, fileindex=IntSlider(0, 0, len(filenames)-1,\n",
    "                                        continuous_update=False))\n",
    "\n",
    "\n",
    "# sorted list of all TIFF files in SPIM dataset\n",
    "SPIM_DATASET_NAME = 'nih3t3-egfp_2'\n",
    "SPIM_FILENAMES = list(sorted(glob.glob(\n",
    "    os.path.join(DATA_PATH, SPIM_DATASET_NAME, 'Pos0', '*.tif'))))\n",
    "\n",
    "browse_images(SPIM_FILENAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that:\n",
    "\n",
    "* the images contain **large areas of background**, which are not of interest for the analysis and can be cropped or masked.\n",
    "\n",
    "* the **shape of the cell changes** or the cell moves out of focus in the second half of the time series. This part will need to be discarded.\n",
    "\n",
    "* the sample shows **strong photobleaching**, i.e. the fluorophore molecules (EGFP) are altered during the acquisition such that it permanently is unable to fluoresce. This decay in intensity needs to be corrected before calculating the pair correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select image regions of interest\n",
    "\n",
    "To select regions of images that contain objects, we use the morphology and segmentation functions of the  [scikit-image](http://scikit-image.org/) library.\n",
    "\n",
    "The image is **smoothed** using a Gaussian filter and then **binarized** by an intensity threshold. Small **holes are closed**, and **border artifacts removed**. Connected **regions are labeled** and sorted by their area. **Small regions are discarded**, and the remaining regions expanded to a multiple of 64 pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "import skimage.filters\n",
    "import skimage.morphology\n",
    "import skimage.segmentation\n",
    "import skimage.restoration\n",
    "\n",
    "import tifffile\n",
    "import matplotlib\n",
    "from matplotlib import pyplot\n",
    "from ipywidgets import interact, IntSlider, FloatSlider\n",
    "\n",
    "\n",
    "# global variable where found regions will be stored\n",
    "REGIONS_FOUND = []\n",
    "\n",
    "\n",
    "def find_regions(filenames, imread=tifffile.imread):\n",
    "    \"\"\"Interactively find regions of interest in image files.\"\"\"\n",
    "\n",
    "    def _plot(fileindex=0, sigma=4.0, threshold=0.0,\n",
    "              closegaps=10, minarea=64*64, pow2size=6):\n",
    "        # read image\n",
    "        image = imread(filenames[fileindex])\n",
    "\n",
    "        # normalize image\n",
    "        image = image.astype('float64')\n",
    "        image -= image.min()\n",
    "        image /= image.max()\n",
    "\n",
    "        # remove noise by smoothing with Gaussian filter\n",
    "        image = skimage.filters.gaussian(image, sigma)\n",
    "\n",
    "        # binarize image with intensity threshold\n",
    "        if threshold == 0.0:\n",
    "            threshold = image.mean()\n",
    "            # skimage.filters offers many threshold_* functions:\n",
    "            #     otsu, li, yen, adaptive, and isodata\n",
    "        binary = image > threshold\n",
    "\n",
    "        # close small gaps\n",
    "        binary = skimage.morphology.closing(\n",
    "            binary, skimage.morphology.square(closegaps))\n",
    "\n",
    "        # remove artifacts connected to image border\n",
    "        skimage.segmentation.clear_border(binary)\n",
    "\n",
    "        # label image regions\n",
    "        labels = skimage.measure.label(binary)\n",
    "\n",
    "        # discard small regions\n",
    "        regions = (r for r in skimage.measure.regionprops(labels)\n",
    "                   if r.area > minarea)\n",
    "\n",
    "        # sort regions by area\n",
    "        regions = reversed(sorted(regions, key=lambda x: x.area))\n",
    "\n",
    "        def expand_bbox(bbox, shape):\n",
    "            # return bounding box expanded to multiple of modulo\n",
    "            minrow, mincol, maxrow, maxcol = bbox\n",
    "            modulo = 2**pow2size\n",
    "            div, mod = divmod(maxrow - minrow, modulo)\n",
    "            if mod:\n",
    "                d = (div + 1) * modulo\n",
    "                minrow = max(0, minrow - (d - maxrow + minrow) // 2)\n",
    "                maxrow = min(shape[0]-1, minrow + d)\n",
    "                minrow = max(0, maxrow - d)\n",
    "            div, mod = divmod(maxcol - mincol, modulo)\n",
    "            if mod:\n",
    "                d = (div + 1) * modulo\n",
    "                mincol = max(0, mincol - (d - maxcol + mincol) // 2)\n",
    "                maxcol = min(shape[1]-1, mincol + d)\n",
    "                mincol = max(0, maxcol - d)\n",
    "            return minrow, mincol, maxrow, maxcol\n",
    "\n",
    "        # keep only bounding box of regions\n",
    "        regions = [expand_bbox(r.bbox, image.shape) for r in regions]\n",
    "\n",
    "        # plot image and regions\n",
    "        pyplot.figure(figsize=(8, 5))\n",
    "        pyplot.imshow(image.T, vmin=0.0, vmax=1.0,\n",
    "                      cmap='viridis', interpolation='lanczos')\n",
    "        ax = pyplot.gca()\n",
    "        for region in regions:\n",
    "            minrow, mincol, maxrow, maxcol = region\n",
    "            rect = matplotlib.patches.Rectangle(\n",
    "                (minrow+1, mincol+1), maxrow-minrow-2, maxcol-mincol-2,\n",
    "                fill=False, edgecolor='red', linewidth=2)\n",
    "            ax.add_patch(rect)\n",
    "        # pyplot.colorbar(orientation='horizontal')\n",
    "        pyplot.show()\n",
    "\n",
    "        # store regions in global variable\n",
    "        global REGIONS_FOUND\n",
    "        REGIONS_FOUND = regions\n",
    "        return regions\n",
    "\n",
    "    interact(_plot,\n",
    "             fileindex=IntSlider(0, min=0, max=len(filenames)-1,\n",
    "                                 continuous_update=False),\n",
    "             sigma=FloatSlider(4, min=0.1, max=16.0, step=0.1,\n",
    "                               continuous_update=False),\n",
    "             threshold=FloatSlider(0.0, min=0.0, max=1.0, step=0.01,\n",
    "                                   continuous_update=False),\n",
    "             closegaps=IntSlider(10, min=1, max=20,\n",
    "                                 continuous_update=False),\n",
    "             minarea=IntSlider(64*64, min=1, max=256*256,\n",
    "                               continuous_update=False),\n",
    "             pow2size=IntSlider(6, min=0, max=8,\n",
    "                                continuous_update=False))\n",
    "\n",
    "\n",
    "find_regions(SPIM_FILENAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save region from SPIM files as chunked HDF5 file\n",
    "\n",
    "For further analysis, we extract the selected region from all TIFF files and save them using a file format that is more efficient for both, image and time series access. Some options are:\n",
    "\n",
    "* a binary file format that can be directly memory-mapped to a numpy array, such as a raw binary file or a [ImageJ hyperstack](https://imagej.nih.gov/ij/docs/guide/146-8.html#fig:Stacks-and-Hyperstacks) compatible TIFF file.\n",
    "\n",
    "* a chunked, optionally compressed, N-dimensional array storage format that allow numpy-like array access such as [HDF5](https://portal.hdfgroup.org/display/HDF5/HDF5) or [zarr](https://zarr.readthedocs.io).\n",
    "\n",
    "\n",
    "![chunking](chunking.png)\n",
    "\n",
    "\n",
    "These memory-mapped and chunked formats have significant advantages over accessing thousands of individual files:\n",
    "\n",
    "1. less overhead than parsing individual TIFF files\n",
    "\n",
    "2. **increased I/O performance** because of compression or operating system optimizations\n",
    "\n",
    "3. small segments of the data can be accessed through the numpy array interface **without reading the entire file into main memory**\n",
    "\n",
    "\n",
    "We will use the [h5py](https://www.h5py.org/) library to save the largest selected region from the first 20,000 images of the time series as a **chunked, uncompressed dataset in a HDF5 file**. Too speed up the process, we use multi-threading and process TIFF files in chunks of 1024 in memory before writing to the HDF5 file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import h5py\n",
    "import tifffile\n",
    "\n",
    "\n",
    "def tiff2hdf5(hdf5file, tifffiles, region,\n",
    "              dataset_name='spim_data', chunks=(512, 16, 16), max_workers=32):\n",
    "    \"\"\"Write image region from TIFF files to chunked dataset in HDF5 file.\"\"\"\n",
    "    minrow, mincol, maxrow, maxcol = region\n",
    "    image = tifffile.imread(tifffiles[0])\n",
    "    nimages = len(tifffiles)\n",
    "    shape = nimages, maxrow - minrow, maxcol - mincol\n",
    "    dtype = image.dtype\n",
    "\n",
    "    with h5py.File(hdf5file, 'w') as hdf:\n",
    "\n",
    "        if dataset_name in hdf:\n",
    "            del hdf[dataset_name]\n",
    "\n",
    "        dataset = hdf.create_dataset(dataset_name, shape=shape, dtype=dtype,\n",
    "                                     chunks=chunks)\n",
    "        dataset.attrs['region'] = region\n",
    "        dataset.attrs['file'] = tifffiles[0]\n",
    "\n",
    "        def convert_chunk(start, size=chunks[0]):\n",
    "            # copy size images from TIFF files to HDF5 dataset\n",
    "            # using a temporary buffer\n",
    "            temp = numpy.empty(\n",
    "                shape=(chunks[0], dataset.shape[1], dataset.shape[2]),\n",
    "                dtype=dataset.dtype)\n",
    "\n",
    "            for index, fname in enumerate(tifffiles[start:start+size]):\n",
    "                image = tifffile.imread(fname, key=0)\n",
    "                temp[index] = image[minrow:maxrow, mincol:maxcol]\n",
    "\n",
    "            dataset[start:start+size] = temp[:index+1]\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers) as executor:\n",
    "            executor.map(convert_chunk, range(0, nimages, chunks[0]))\n",
    "\n",
    "\n",
    "def cleanup_hdf(remove=False):\n",
    "    \"\"\"Close handles and optionally remove existing HDF5 file.\"\"\"\n",
    "    try:\n",
    "        HDF5_FILE.flush()\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        del SPIM_DATASET\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        del SPIM_IPCF_RESULT\n",
    "    except Exception:\n",
    "        pass            \n",
    "    try:\n",
    "        HDF5_FILE.close()\n",
    "    except Exception:\n",
    "        pass\n",
    "    if remove:\n",
    "        try:\n",
    "            os.remove(HDF5_FILENAME)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "HDF5_FILENAME = os.path.join(SCRATCH_PATH, SPIM_DATASET_NAME) + '.hdf5'\n",
    "\n",
    "cleanup_hdf(remove=True)\n",
    "\n",
    "%time tiff2hdf5(HDF5_FILENAME, SPIM_FILENAMES[1000:21000], REGIONS_FOUND[0])\n",
    "\n",
    "print('{}  ({:.1f} GB)'.format(HDF5_FILENAME,\n",
    "                               os.path.getsize(HDF5_FILENAME) / 1024**3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chunked 3D dataset stored in the HDF5 file can be accessed transparently with a numpy ndarray style API. No data is read from the file until the array is indexed or sliced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# the HDF5 file will stay open until the end of the document\n",
    "HDF5_FILE = h5py.File(HDF5_FILENAME, 'r+')\n",
    "\n",
    "SPIM_DATASET = HDF5_FILE['spim_data']\n",
    "\n",
    "# print information about dataset\n",
    "print('shape: ', SPIM_DATASET.shape)\n",
    "print('dtype: ', SPIM_DATASET.dtype)\n",
    "print('file:  ', SPIM_DATASET.attrs['file'])\n",
    "print('region:', SPIM_DATASET.attrs['region'])\n",
    "print()\n",
    "\n",
    "# slicing datasets returns a numpy array in memory\n",
    "print('reading a single image:')\n",
    "%time image = SPIM_DATASET[10000]\n",
    "print(image.shape)\n",
    "print()\n",
    "\n",
    "print('reading a single time series:')\n",
    "%time timeseries = SPIM_DATASET[:, 400, 100]\n",
    "print(timeseries.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's interactively plot images in the spim_data dataset in the HDF5 file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "\n",
    "def imshow_ts(image_timeseries, vmin=0, vmax=None):\n",
    "    \"\"\"Interactively plot images in time series.\"\"\"\n",
    "\n",
    "    def _plot(index=0):\n",
    "        image = image_timeseries[index]\n",
    "        pyplot.figure(figsize=(8, 5))\n",
    "        pyplot.imshow(image.T, vmin=vmin, vmax=vmax, \n",
    "                      cmap='viridis', interpolation='lanczos')\n",
    "        pyplot.colorbar(orientation='horizontal')\n",
    "        pyplot.show()\n",
    "\n",
    "    interact(_plot, \n",
    "             index=IntSlider(0, 0, image_timeseries.shape[0]-1, 100, \n",
    "                             continuous_update=False))\n",
    "\n",
    "\n",
    "imshow_ts(SPIM_DATASET)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct for photobleaching\n",
    "\n",
    "As noted before, the sample shows strong photobleaching, which needs to be corrected before correlation analysis.\n",
    "\n",
    "The exponential decay of the fluorescence intensity due to photobleaching is visualized by plotting the time series of selected pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "\n",
    "def plot_its(image_timeseries, ymax=None):\n",
    "    \"\"\"Interactively plot time series at selected pixel.\"\"\"\n",
    "    ntimes, height, width = image_timeseries.shape\n",
    "    pow2 = int(math.log(ntimes, 2))\n",
    "    t = numpy.arange(ntimes)\n",
    "\n",
    "    def _plot(y, x, start=0, pow2=pow2):\n",
    "        pyplot.figure(figsize=(6, 4))\n",
    "        pyplot.title('time series')\n",
    "        pyplot.xlabel('time index')\n",
    "        pyplot.ylabel('intensity')\n",
    "        stop = min(start + 2**pow2, ntimes)\n",
    "        pyplot.plot(t[start:stop], image_timeseries[start:stop, y, x])\n",
    "        pyplot.gca().set_xlim([start, stop-1])\n",
    "        pyplot.gca().set_ylim([0, ymax])\n",
    "        pyplot.show()\n",
    "\n",
    "    interact(_plot,\n",
    "             y=IntSlider(height//2, 0, height-1, continuous_update=False),\n",
    "             x=IntSlider(width//2, 0, width-1, continuous_update=False),\n",
    "             start=IntSlider(0, 0, ntimes-2**4, 100, continuous_update=False),\n",
    "             pow2=IntSlider(pow2, 4, pow2, 1, continuous_update=False))\n",
    "\n",
    "\n",
    "plot_its(SPIM_DATASET, ymax=2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To correct for photobleaching, we **subtract the smoothed time series from themselves**, **add the signal mean**, and **correct for differences in intensity fluctuations**. \n",
    "\n",
    "The double exponential smoothing algorithm is the same as used for smoothing the log-binned cross-correlation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython --compile-args=-O2  $OPENMP_ARGS\n",
    "#\n",
    "#cython: boundscheck=False\n",
    "#cython: wraparound=False\n",
    "#cython: cdivision=True\n",
    "\n",
    "import numpy\n",
    "from cython.parallel import prange, parallel\n",
    "\n",
    "cimport numpy\n",
    "from libc.stdlib cimport malloc, free\n",
    "from libc.math cimport sqrt, fabs, round\n",
    "\n",
    "ctypedef numpy.uint16_t uint16_t\n",
    "\n",
    "\n",
    "cdef void highpass_filter(uint16_t[:] data, uint16_t[::1] out,\n",
    "                          double *a, ssize_t size, ssize_t filtersize) nogil:\n",
    "    \"\"\"Subtract smoothed, add mean, and correct deficit.\n",
    "\n",
    "    Using double exponential smoothing with factor 1/filtersize.\n",
    "\n",
    "    \"\"\"\n",
    "    cdef:\n",
    "        ssize_t i\n",
    "        ssize_t sumd\n",
    "        double t\n",
    "        uint16_t d\n",
    "        double f0 = 1.0 / <double>filtersize\n",
    "        double f1 = 1.0 - f0\n",
    "        double mean\n",
    "        double deficit\n",
    "\n",
    "    sumd = data[0]\n",
    "    a[0] = <double>data[0]\n",
    "    for i in range(1, size):\n",
    "        d = data[i]\n",
    "        sumd += d\n",
    "        a[i] = <double>d * f0 + a[i-1] * f1\n",
    "    for i in range(size-2, -1, -1):\n",
    "        a[i] = a[i] * f0 + a[i+1] * f1\n",
    "\n",
    "    mean = sumd / size\n",
    "\n",
    "    for i in range(size):\n",
    "        t = <double>data[i]\n",
    "        deficit = sqrt(fabs(mean / a[i]))\n",
    "        t = round(deficit * (t - a[i]) + mean)\n",
    "        if t < 0.5:\n",
    "            t = 0.0\n",
    "        elif t > 65534.5:\n",
    "            t = 65535.0\n",
    "        else:\n",
    "            t = t + 0.5\n",
    "        out[i] = <uint16_t>t\n",
    "\n",
    "\n",
    "def correct_bleaching(numpy.ndarray[uint16_t, ndim=3] image_timeseries,\n",
    "                      ssize_t filtersize=1024, int num_threads=0):\n",
    "    \"\"\"Return time series for exponential photobleaching.\n",
    "\n",
    "    The first and last 'filtersize' samples of 'image_timeseries' \n",
    "    are removed.\n",
    "\n",
    "    \"\"\"\n",
    "    cdef:\n",
    "        uint16_t[:, :, ::1] out\n",
    "        uint16_t[:, :, ::] data = image_timeseries\n",
    "        ssize_t ntimes = data.shape[0]\n",
    "        ssize_t height = data.shape[1]\n",
    "        ssize_t width = data.shape[2]\n",
    "        ssize_t t, y, x\n",
    "        double *a_\n",
    "        \n",
    "    if filtersize <= 0:\n",
    "        return image_timeseries\n",
    "\n",
    "    # allocate output array with contiguous time axis\n",
    "    result = numpy.empty((height, width, ntimes), image_timeseries.dtype)\n",
    "    out = result\n",
    "\n",
    "    with nogil, parallel(num_threads=num_threads):\n",
    "\n",
    "        # thread-local input/output data\n",
    "        a_ = <double *>malloc(sizeof(double) * ntimes)\n",
    "        if a_ == NULL:\n",
    "            with gil:\n",
    "                raise MemoryError('could not allocate a_')\n",
    "\n",
    "        for y in prange(height):\n",
    "            for x in range(width):\n",
    "                highpass_filter(data[:, y, x], out[y, x, :],\n",
    "                                a_, ntimes, filtersize)\n",
    "\n",
    "        free(a_)\n",
    "\n",
    "    result = numpy.moveaxis(result, -1, 0)\n",
    "    result = result[filtersize:-filtersize]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the input and output data will likely not fit in RAM, we apply the photobleaching correction on a small part of the image for testing and visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time corrected = correct_bleaching(SPIM_DATASET[:, 400:432, 80:112])\n",
    "\n",
    "plot_its(corrected, ymax=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-core image analysis using Dask\n",
    "\n",
    "The multi-gigabyte SPIM image time series is too large to be analyzed by our implementation of the image pCF analysis and photobleaching correction functions because it likely **exceeds the available main memory on our personal computer**.\n",
    "\n",
    "The [Dask](http://dask.pydata.org) library can be used to **chop the big image in smaller blocks/chunks** and **schedule the analysis of individual blocks** based on available memory, CPU cores or cluster nodes. \n",
    "\n",
    "Dask also supports **overlapping blocks** where the borders between neighboring blocks overlap, as required for the ipCF analysis:\n",
    "\n",
    "![compute_graph](compute_graph.png)\n",
    "\n",
    "**Operations on dask arrays are lazy and are queued**. No computations are performed until values are requested to be computed. \n",
    "\n",
    "Dask can work with **arrays on disks** such as arrays stored in HDF5 and memory-mapped binary files. Only when computations are performed are data loaded into memory.\n",
    "\n",
    "Dask provides **serial, threaded, multiprocessing, and distributed schedulers** that scale from laptop computers to clusters of computers. The default scheduler is threaded, which works well for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.array\n",
    "\n",
    "\n",
    "def run_ipcf_blocked(image_timeseries, output=None, chunks=(32, 32),\n",
    "                     radius=6, npoints=32, nbins=32, filtersize=0,\n",
    "                     ipcf_function=ipcf_cython,\n",
    "                     correct_bleaching=correct_bleaching,\n",
    "                     num_workers=MAXCPUS, num_threads=2):\n",
    "    \"\"\"Run ipcf_function on small overlapping blocks of image timeseries.\"\"\"\n",
    "    ntimes, height, width = image_timeseries.shape\n",
    "\n",
    "    # truncate time axis to power of two\n",
    "    ntimes = 2**int(math.log(ntimes-2*filtersize, 2))\n",
    "\n",
    "    # calculate circle coordinates\n",
    "    circle_coordinates = circle(radius, npoints)\n",
    "\n",
    "    # calculate log-bins\n",
    "    bins = logbins(ntimes // 2, nbins)\n",
    "    nbins = bins.size\n",
    "    \n",
    "    # create a dask chunked array\n",
    "    blocks = dask.array.from_array(image_timeseries,\n",
    "                                   chunks=(image_timeseries.shape[0],\n",
    "                                           chunks[0], chunks[1]))\n",
    "\n",
    "    # correct bleaching on overlapping blocks\n",
    "    corrected = blocks.map_overlap(\n",
    "        correct_bleaching,  \n",
    "        depth=(0, radius, radius),\n",
    "        boundary=(0, 1, 1),\n",
    "        trim=False,\n",
    "        dtype=blocks.dtype,\n",
    "        filtersize=filtersize, \n",
    "        num_threads=num_threads)\n",
    "\n",
    "    # ipfc on corrected, overlapping blocks\n",
    "    ipcf_result = corrected.map_blocks(\n",
    "        ipcf_function,\n",
    "        dtype='float32',\n",
    "        chunks=(chunks[0], chunks[1], npoints, nbins),\n",
    "        drop_axis=0,\n",
    "        new_axis=[2, 3],\n",
    "        circle_coordinates=circle_coordinates, \n",
    "        bins=bins,\n",
    "        num_threads=num_threads)\n",
    "\n",
    "    # execute the compute graph or determine shape of result\n",
    "    if output == 'shape':\n",
    "        output = ipcf_result.shape\n",
    "    elif output is None:\n",
    "        output = ipcf_result.compute(num_workers=num_workers)\n",
    "    else:\n",
    "        ipcf_result.store(output, num_workers=num_workers)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `run_ipcf_blocked` function returns the computed results as a numpy array in memory by default. For larger datasets, the block results can be written incrementally to a memory-mapped array on disk or a HDF5 dataset (see below).\n",
    "\n",
    "We test the blocked ipCF analysis on the simulated dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 6\n",
    "\n",
    "%time ipcf_result = run_ipcf_blocked(SIMULATION_DATA, radius=radius)\n",
    "\n",
    "test_ipcf(ipcf_result[radius:-radius, radius:-radius])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run pair correlation function analysis on SPIM image time series\n",
    "\n",
    "Finally, we run the ipCF analysis on the region of interest extracted from the SPIM images and corrected for photobleaching.\n",
    "\n",
    "We'll use chunks of 64x64 pixels, half of the CPU cores for dask worker threads, and 2 threads for OpenMP. The analysis of each chunk uses ~400 MB RAM and the result array uses 540 MB. The size of chunks and number of worker vs OpenMP threads should be adjusted based on the image size, available main memory, and CPU cores.\n",
    "\n",
    "The blocks of the ipCF analysis results are directly saved into a dataset in the same HDF5 file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict(\n",
    "    radius = 6,\n",
    "    npoints = 32,\n",
    "    nbins = 32,\n",
    "    filtersize = 1024,\n",
    "    chunks = (64, 64),\n",
    "    ipcf_function=ipcf_cython,\n",
    "    num_workers = MAXCPUS,\n",
    "    num_threads = 2)\n",
    "\n",
    "# remove previous results\n",
    "try:\n",
    "    del SPIM_IPCF_RESULT\n",
    "except Exception:\n",
    "    pass\n",
    "if 'ipcf_result' in HDF5_FILE:\n",
    "    del HDF5_FILE['ipcf_result']\n",
    "\n",
    "# determine size of output array\n",
    "shape = run_ipcf_blocked(SPIM_DATASET, 'shape')\n",
    "\n",
    "# allocate a new dataset in the HDF5 file\n",
    "output = HDF5_FILE.create_dataset('ipcf_result', shape=shape, dtype='float32')\n",
    "\n",
    "# run the analysis with output to HDF5 dataset\n",
    "%time SPIM_IPCF_RESULT = run_ipcf_blocked(SPIM_DATASET, output, **args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 4D result array can be visualized using the previously defined interactive `plot_ipcf_sprites` and `plot_ipcf_images` functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ipcf_sprites(SPIM_IPCF_RESULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ipcf_images(SPIM_IPCF_RESULT, figsize=(10, 6), interpolation='lanczos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "Close the HDF5 file when the datasets are no longer needed in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_hdf(remove=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Outlook\n",
    "\n",
    "In this tutorial, we developed code to perform pair correlation function analysis of a big image time series (3D) and ran it on a personal computer. \n",
    "\n",
    "We may now move on to:\n",
    "\n",
    "* analyze and visualize the ipCF results with [Globals for Images Â· SimFCS](https://www.lfd.uci.edu/globals/) software\n",
    "* skip computing pair correlation in uninteresting regions using a pixel mask\n",
    "* improve error handling, testing, and documentation\n",
    "* create a Python package, which can be imported into other modules\n",
    "* create a dynamic C library, which can be used from Python and other programming languages\n",
    "* implement 3D pair- and cross-channel correlation function analysis of 5D data\n",
    "* explore 2D or 3D correlation techniques (lSTICS, iMSD)\n",
    "* analyze bigger image time series on a cluster using dask/distributed\n",
    "* implement the ipCF algorithm as a CUDA kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Image pair correlation function analysis\n",
    "\n",
    "* Carmine Di Rienzo, Enrico Gratton, Fabio Beltram, and Francesco Cardarelli. [Spatiotemporal fluctuation analysis: a powerful tool for the future nanoscopy of molecular processes](https://doi.org/10.1016/j.bpj.2016.07.015). Biophys J. 2016; 111(4): 679-685. PMCID: PMC5002078\n",
    "\n",
    "* Carmine Di Rienzo, Francesco Cardarelli, Mariagrazia Di Luca, Fabio Beltram, and Enrico Gratton. [Diffusion tensor analysis by two-dimensional pair correlation of fluorescence fluctuations in cells](https://doi.org/10.1016/j.bpj.2016.07.005). Biophys J. 2016; 111(4): 841-851. PMCID: PMC5002073\n",
    "\n",
    "* Enrico Gratton. [Globals software tutorial - 2D pair correlation function analysis](http://www.lfd.uci.edu/globals/tutorials/) (2016) [Online]\n",
    "\n",
    "* Enrico Gratton. [LFD Workshop 2016 lecture 6: The pair correlation approach](http://www.lfd.uci.edu/workshop/2016/) (2016) [Online]\n",
    "\n",
    "### Scientific Computing in Python\n",
    "\n",
    "* Python Software Foundation. [The Python Programming Language](https://www.python.org) (2016) [Online]\n",
    "\n",
    "* Charles R Harris et al. [Array programming with NumPy](https://dx.doi.org/10.1038/s41586-020-2649-2). *Nature*, 585(7825), 357-362 (2020)\n",
    "\n",
    "* John D. Hunter. [Matplotlib: A 2D Graphics Environment](https://doi.org/10.1109/MCSE.2007.55), *Computing in Science & Engineering*, 9, 90-95 (2007)\n",
    "\n",
    "* Jones E, Oliphant E, Peterson P, et al. [SciPy: Open Source Scientific Tools for Python](https://www.scipy.org/) (2016) [Online]\n",
    "\n",
    "* Siu Kwan Lam, Antoine Pitrou, and Stanley Seibert. [Numba: A LLVM-based Python JIT Compiler](https://doi.org/10.1145/2833157.2833162). *Proceedings of the Second Workshop on the LLVM Compiler Infrastructure in HPC*. Article No. 7 (2015)\n",
    "\n",
    "* Fernando PÃ©rez and Brian E. Granger. [IPython: A System for Interactive Scientific Computing](https://doi.org/10.1109/MCSE.2007.53). *Computing in Science & Engineering*, 9, 21-29 (2007)\n",
    "\n",
    "* Stefan Behnel, Robert Bradshaw, Craig Citro, Lisandro Dalcin, Dag Sverre Seljebotn and Kurt Smith. [Cython: The Best of Both Worlds](https://doi.org/10.1109/MCSE.2010.118), *Computing in Science and Engineering*, 13, 31-39 (2011)\n",
    "\n",
    "* StÃ©fan van der Walt, Johannes L. SchÃ¶nberger, Juan Nunez-Iglesias, FranÃ§ois Boulogne, Joshua D. Warner, Neil Yager, Emmanuelle Gouillart, Tony Yu and the scikit-image contributors. [scikit-image: Image processing in Python](https://doi.org/10.7717/peerj.453). *PeerJ* 2:e453 (2014)\n",
    "\n",
    "* Matthew Rocklin. [Dask: Parallel Computation with Blocked algorithms and Task Scheduling](http://conference.scipy.org/proceedings/scipy2015/matthew_rocklin.html). *Proceedings of the 14th Python in Science Conference*, Scipy2015 (2015)\n",
    "\n",
    "* Andrew Collette. [Python and HDF5](https://shop.oreilly.com/product/0636920030249.do). OâReilly book (2013)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System information\n",
    "\n",
    "Print information about hardware and software used to generate this document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext Cython\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import shutil\n",
    "import datetime\n",
    "import multiprocessing\n",
    "import notebook\n",
    "from distutils import ccompiler\n",
    "\n",
    "print(sys.executable)\n",
    "print('Python', sys.version, end='\\n\\n')\n",
    "\n",
    "for module in ('IPython notebook ipywidgets widgetsnbextension numpy '\n",
    "               'scipy matplotlib skimage numba cupy h5py Cython dask '\n",
    "               'tifffile'.split()):\n",
    "    try:\n",
    "        __import__(module)\n",
    "    except Exception:\n",
    "        continue\n",
    "    lib = sys.modules[module]\n",
    "    print(module.lower(), getattr(lib, '__version__', 'Unknown'))\n",
    "\n",
    "print('\\nCompiler type:', ccompiler.new_compiler().compiler_type, end='\\n\\n')\n",
    "print(multiprocessing.cpu_count(), 'CPU cores')\n",
    "\n",
    "try:\n",
    "    import psutil\n",
    "    print('{:.0f} GB main memory\\n'.format(psutil.virtual_memory()[0]/2**30))\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import numba.cuda\n",
    "    print(numba.cuda.gpus[0].name.decode('utf8'))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if shutil.which('nvcc'):\n",
    "    print()\n",
    "    !nvcc --version\n",
    "\n",
    "print()\n",
    "try:\n",
    "    print('Duration:', datetime.datetime.now() - START_TIME)\n",
    "    print()\n",
    "except NameError:\n",
    "    pass\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
